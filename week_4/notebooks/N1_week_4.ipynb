{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","machine_shape":"hm","authorship_tag":"ABX9TyO1bUaWk9uaUNwjl6n9ESyF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F7GIAnQyE-OR","executionInfo":{"status":"ok","timestamp":1762347318657,"user_tz":-300,"elapsed":21681,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"70cd0807-486a-477a-b6e5-c107eeae8d23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# /colab_notebooks/week4/main_week4.py\n","# Week 4 — Clean-slate baseline training with visible results.\n","\n","from __future__ import annotations\n","\n","import json\n","import os\n","import re\n","import warnings\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Tuple\n","\n","import joblib\n","import numpy as np\n","import pandas as pd\n","from sklearn.compose import ColumnTransformer\n","from sklearn.exceptions import ConvergenceWarning\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import (\n","    accuracy_score,\n","    classification_report,\n","    confusion_matrix,\n","    f1_score,\n","    log_loss,\n","    precision_score,\n","    recall_score,\n","    roc_auc_score,\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","\n","\n","# ----------------------------- Config ----------------------------- #\n","\n","DRIVE_MOUNT_PT: str = \"/content/drive\"\n","PROJECT_ROOT: str = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","W3_TRAIN_DIR: str = os.path.join(PROJECT_ROOT, \"week_3\", \"outputs\", \"training_data\")\n","W4_OUT: str = os.path.join(PROJECT_ROOT, \"week_4\", \"outputs\")\n","MODELS_DIR: str = os.path.join(W4_OUT, \"models\")\n","METRICS_DIR: str = os.path.join(W4_OUT, \"metrics\")\n","REPORTS_DIR: str = os.path.join(W4_OUT, \"reports\")\n","TABLES_DIR: str = os.path.join(W4_OUT, \"tables\")\n","\n","DEFAULT_CFG: Dict = {\n","    \"target\": \"label\",\n","    \"id_column\": None,\n","    \"test_size\": 0.2,\n","    \"random_state\": 42,\n","    \"class_weight\": \"balanced\",\n","    \"n_jobs\": -1,\n","    \"max_categories\": 500,\n","    \"rf\": {\"n_estimators\": 300, \"max_depth\": None, \"min_samples_leaf\": 1},\n","    \"logreg\": {\"C\": 1.0, \"max_iter\": 200},\n","}\n","\n","\n","# ----------------------------- Utilities ----------------------------- #\n","\n","@dataclass(frozen=True)\n","class TrainArtifact:\n","    domain: str\n","    path: str\n","\n","\n","def _ensure_io() -> None:\n","    if not os.path.isdir(W3_TRAIN_DIR):\n","        raise FileNotFoundError(f\"Missing training data dir: {W3_TRAIN_DIR}\")\n","    for d in [W4_OUT, MODELS_DIR, METRICS_DIR, REPORTS_DIR, TABLES_DIR]:\n","        os.makedirs(d, exist_ok=True)\n","\n","\n","def _load_config() -> Dict:\n","    cfg_path = os.path.join(W4_OUT, \"config.json\")\n","    cfg = DEFAULT_CFG.copy()\n","    if os.path.isfile(cfg_path):\n","        on_disk = json.load(open(cfg_path))\n","        if not isinstance(on_disk, dict):\n","            raise ValueError(\"config.json must be a JSON object\")\n","        for k, v in on_disk.items():\n","            if isinstance(v, dict) and isinstance(cfg.get(k), dict):\n","                cfg[k].update(v)\n","            else:\n","                cfg[k] = v\n","    return cfg\n","\n","\n","def _discover_training_sets() -> List[TrainArtifact]:\n","    arts: List[TrainArtifact] = []\n","    for fname in sorted(os.listdir(W3_TRAIN_DIR)):\n","        if fname.startswith(\"train_ready_\") and fname.endswith(\".parquet\"):\n","            domain = re.sub(r\"^train_ready_(.+)\\.parquet$\", r\"\\1\", fname)\n","            arts.append(TrainArtifact(domain=domain, path=os.path.join(W3_TRAIN_DIR, fname)))\n","    if not arts:\n","        raise FileNotFoundError(f\"No train_ready_*.parquet under {W3_TRAIN_DIR}\")\n","    return arts\n","\n","\n","def _split_features(df: pd.DataFrame, target: str, id_column: Optional[str]) -> Tuple[pd.DataFrame, pd.Series]:\n","    if target not in df.columns:\n","        raise KeyError(f\"Target column '{target}' not found.\")\n","    y = df[target]\n","    drop_cols = [target]\n","    if id_column and id_column in df.columns:\n","        drop_cols.append(id_column)\n","    X = df.drop(columns=drop_cols)\n","    return X, y\n","\n","\n","def _drop_constants(df: pd.DataFrame) -> pd.DataFrame:\n","    nunique = df.nunique(dropna=False)\n","    const = nunique[nunique <= 1].index.tolist()\n","    return df.drop(columns=const) if const else df\n","\n","\n","def _infer_types(X: pd.DataFrame) -> Tuple[List[str], List[str]]:\n","    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n","    cat_cols = [c for c in X.columns if c not in num_cols]\n","    return num_cols, cat_cols\n","\n","\n","def _build_pre(num_cols: List[str], cat_cols: List[str], max_categories: int) -> ColumnTransformer:\n","    num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n","    cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n","                         (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, max_categories=max_categories))])\n","    return ColumnTransformer([(\"num\", num_pipe, num_cols), (\"cat\", cat_pipe, cat_cols)], remainder=\"drop\")\n","\n","\n","def _build_models(pre: ColumnTransformer, cfg: Dict) -> Dict[str, Pipeline]:\n","    logreg = Pipeline([\n","        (\"pre\", pre),\n","        (\"clf\", LogisticRegression(\n","            solver=\"saga\",\n","            penalty=\"l2\",\n","            C=float(cfg[\"logreg\"][\"C\"]),\n","            max_iter=int(cfg[\"logreg\"][\"max_iter\"]),\n","            n_jobs=int(cfg[\"n_jobs\"]),\n","            class_weight=cfg[\"class_weight\"],\n","            random_state=int(cfg[\"random_state\"]),\n","        )),\n","    ])\n","    rf = Pipeline([\n","        (\"pre\", pre),\n","        (\"clf\", RandomForestClassifier(\n","            n_estimators=int(cfg[\"rf\"][\"n_estimators\"]),\n","            max_depth=cfg[\"rf\"][\"max_depth\"],\n","            min_samples_leaf=int(cfg[\"rf\"][\"min_samples_leaf\"]),\n","            n_jobs=int(cfg[\"n_jobs\"]),\n","            class_weight=cfg[\"class_weight\"],\n","            random_state=int(cfg[\"random_state\"]),\n","        )),\n","    ])\n","    return {\"logreg\": logreg, \"rf\": rf}\n","\n","\n","def _safe_roc_auc(y_true: pd.Series, y_proba: Optional[np.ndarray]) -> Optional[float]:\n","    try:\n","        if y_proba is None:\n","            return None\n","        if y_proba.ndim == 1 or y_proba.shape[1] == 1:\n","            return None\n","        if y_proba.shape[1] == 2:\n","            return float(roc_auc_score(y_true, y_proba[:, 1]))\n","        return float(roc_auc_score(y_true, y_proba, multi_class=\"ovr\", average=\"weighted\"))\n","    except Exception:\n","        return None\n","\n","\n","def _eval(y_true: pd.Series, y_pred: np.ndarray, y_proba: Optional[np.ndarray]) -> Dict[str, Optional[float]]:\n","    m: Dict[str, Optional[float]] = {\n","        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n","        \"precision_weighted\": float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n","        \"recall_weighted\": float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n","        \"f1_weighted\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n","        \"roc_auc\": _safe_roc_auc(y_true, y_proba),\n","        \"log_loss\": None,\n","    }\n","    try:\n","        if y_proba is not None:\n","            m[\"log_loss\"] = float(log_loss(y_true, y_proba, labels=np.unique(y_true)))\n","    except Exception:\n","        pass\n","    return m\n","\n","\n","def _classification_report_df(y_true: pd.Series, y_pred: np.ndarray) -> pd.DataFrame:\n","    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n","    return pd.DataFrame(rep).transpose()\n","\n","\n","def _persist(domain: str, model_name: str, pipe: Pipeline, metrics: Dict[str, Optional[float]], report_df: pd.DataFrame) -> None:\n","    joblib.dump(pipe, os.path.join(MODELS_DIR, f\"{domain}_{model_name}.joblib\"))\n","    with open(os.path.join(METRICS_DIR, f\"{domain}_{model_name}.json\"), \"w\") as f:\n","        json.dump(metrics, f, indent=2)\n","    report_df.to_csv(os.path.join(REPORTS_DIR, f\"{domain}_{model_name}_classification_report.csv\"), index=True)\n","\n","\n","def _train_one(df: pd.DataFrame, domain: str, cfg: Dict) -> List[Dict[str, object]]:\n","    df = _drop_constants(df.copy())\n","    X, y = _split_features(df, cfg[\"target\"], cfg.get(\"id_column\"))\n","    num_cols, cat_cols = _infer_types(X)\n","    pre = _build_pre(num_cols, cat_cols, int(cfg[\"max_categories\"]))\n","    models = _build_models(pre, cfg)\n","\n","    X_tr, X_te, y_tr, y_te = train_test_split(\n","        X, y, test_size=float(cfg[\"test_size\"]), random_state=int(cfg[\"random_state\"]), stratify=y\n","    )\n","\n","    rows: List[Dict[str, object]] = []\n","    for name, pipe in models.items():\n","        with warnings.catch_warnings():\n","            warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n","            pipe.fit(X_tr, y_tr)\n","\n","        y_pred = pipe.predict(X_te)\n","        try:\n","            y_proba = pipe.predict_proba(X_te)\n","        except Exception:\n","            y_proba = None\n","\n","        met = _eval(y_te, y_pred, y_proba)\n","        cm = confusion_matrix(y_te, y_pred)\n","        rep_df = _classification_report_df(y_te, y_pred)\n","\n","        _persist(domain, name, pipe, {**met, \"n_test\": int(len(y_te))}, rep_df)\n","\n","        rows.append({\n","            \"domain\": domain,\n","            \"model\": name,\n","            **met,\n","            \"n_train\": int(len(y_tr)),\n","            \"n_test\": int(len(y_te)),\n","            \"n_features_raw\": int(X.shape[1]),\n","            \"n_num\": int(len(num_cols)),\n","            \"n_cat\": int(len(cat_cols)),\n","            \"classes\": \",\".join(map(str, np.unique(y))),\n","            \"confusion_matrix\": \";\".join(\",\".join(map(str, r)) for r in cm),\n","        })\n","    return rows\n","\n","\n","def _write_results_markdown(runs_df: pd.DataFrame, leaderboard: pd.DataFrame, out_path: str) -> None:\n","    lines: List[str] = []\n","    lines.append(\"# Week 4 Results\\n\")\n","    lines.append(\"## Leaderboard (best per domain)\\n\")\n","    lines.append(leaderboard.to_markdown(index=False))\n","    lines.append(\"\\n## All Runs\\n\")\n","    show_cols = [\"domain\", \"model\", \"f1_weighted\", \"accuracy\", \"precision_weighted\", \"recall_weighted\", \"roc_auc\", \"log_loss\", \"n_train\", \"n_test\"]\n","    lines.append(runs_df[show_cols].sort_values([\"domain\", \"f1_weighted\", \"accuracy\"], ascending=[True, False, False]).to_markdown(index=False))\n","    lines.append(\"\\n---\\n**Next**: inspect per-model CSV reports under `week_4/outputs/reports/`.\\n\")\n","    with open(out_path, \"w\") as f:\n","        f.write(\"\\n\".join(lines))\n","\n","\n","def main() -> None:\n","    _ensure_io()\n","    cfg = _load_config()\n","    arts = _discover_training_sets()\n","\n","    all_rows: List[Dict[str, object]] = []\n","    for art in arts:\n","        print(f\"[Week4] Training on {art.domain}\")\n","        df = pd.read_parquet(art.path)\n","        all_rows.extend(_train_one(df, art.domain, cfg))\n","\n","    runs_df = pd.DataFrame(all_rows)\n","    runs_csv = os.path.join(TABLES_DIR, \"runs.csv\")\n","    runs_df.to_csv(runs_csv, index=False)\n","\n","    leaderboard = (\n","        runs_df.sort_values([\"domain\", \"f1_weighted\", \"accuracy\"], ascending=[True, False, False])\n","        .groupby(\"domain\", as_index=False)\n","        .first()\n","    )\n","    lb_csv = os.path.join(TABLES_DIR, \"leaderboard.csv\")\n","    leaderboard.to_csv(lb_csv, index=False)\n","\n","    results_md = os.path.join(W4_OUT, \"results.md\")\n","    _write_results_markdown(runs_df, leaderboard, results_md)\n","\n","    print(\"\\n[Week4] Artifacts:\")\n","    print(f\" - models:   {MODELS_DIR}\")\n","    print(f\" - metrics:  {METRICS_DIR}\")\n","    print(f\" - reports:  {REPORTS_DIR}\")\n","    print(f\" - tables:   {TABLES_DIR}\")\n","    print(f\" - summary:  {results_md}\")\n","    print(\"[Week4] Done.\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4sVFvk47rbP","executionInfo":{"status":"ok","timestamp":1762348255473,"user_tz":-300,"elapsed":934008,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"99ee7ee3-5e19-4dd5-f53c-d7610a7f983d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Week4] Training on ALL\n","[Week4] Training on IoT\n","[Week4] Training on Linux\n","[Week4] Training on Network\n","[Week4] Training on Windows\n","\n","[Week4] Artifacts:\n"," - models:   /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_4/outputs/models\n"," - metrics:  /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_4/outputs/metrics\n"," - reports:  /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_4/outputs/reports\n"," - tables:   /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_4/outputs/tables\n"," - summary:  /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_4/outputs/results.md\n","[Week4] Done.\n"]}]},{"cell_type":"code","source":["# /colab_notebooks/week4/main_week4.py\n","# Week 4 — Baseline training with clean code (PEP8, type hints, logging), single file.\n","\n","from __future__ import annotations\n","\n","import json\n","import logging\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Dict, Iterable, List, Optional, Sequence, Tuple\n","\n","import joblib\n","import numpy as np\n","import pandas as pd\n","from sklearn.compose import ColumnTransformer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.impute import SimpleImputer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import (\n","    accuracy_score,\n","    classification_report,\n","    confusion_matrix,\n","    f1_score,\n","    log_loss,\n","    precision_score,\n","    recall_score,\n","    roc_auc_score,\n",")\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import OneHotEncoder, StandardScaler\n","\n","\n","# ---------------------------------------------------------------------------\n","# Configuration\n","# ---------------------------------------------------------------------------\n","\n","DRIVE_MOUNT_PT = Path(\"/content/drive\")\n","PROJECT_ROOT = DRIVE_MOUNT_PT / \"MyDrive\" / \"Colab Notebooks\" / \"New_cyber_project\"\n","\n","W3_TRAIN_DIR = PROJECT_ROOT / \"week_3\" / \"outputs\" / \"training_data\"\n","W4_OUT = PROJECT_ROOT / \"week_4\" / \"outputs\"\n","MODELS_DIR = W4_OUT / \"models\"\n","METRICS_DIR = W4_OUT / \"metrics\"\n","REPORTS_DIR = W4_OUT / \"reports\"\n","TABLES_DIR = W4_OUT / \"tables\"\n","RESULTS_MD = W4_OUT / \"results.md\"\n","\n","\n","@dataclass(frozen=True)\n","class TrainFile:\n","    \"\"\"Immutable descriptor of a train-ready dataset.\"\"\"\n","    domain: str\n","    path: Path\n","\n","\n","@dataclass(frozen=True)\n","class TrainConfig:\n","    \"\"\"Training configuration (WHY: explicit, typed settings to avoid hidden defaults).\"\"\"\n","    target: str = \"label\"\n","    id_column: Optional[str] = None\n","    test_size: float = 0.20\n","    random_state: int = 42\n","    class_weight: Optional[str] = \"balanced\"\n","    n_jobs: int = -1\n","    max_categories: int = 500\n","    rf_n_estimators: int = 300\n","    rf_max_depth: Optional[int] = None\n","    rf_min_samples_leaf: int = 1\n","    logreg_C: float = 1.0\n","    logreg_max_iter: int = 200\n","\n","\n","# ---------------------------------------------------------------------------\n","# Logging\n","# ---------------------------------------------------------------------------\n","\n","def setup_logging() -> None:\n","    \"\"\"Set structured, concise logging (WHY: mentor-friendly visibility).\"\"\"\n","    fmt = \"%(asctime)s | %(levelname)s | %(message)s\"\n","    logging.basicConfig(level=logging.INFO, format=fmt)\n","    # Silence verbose deps\n","    for noisy in (\"urllib3\", \"fsspec\", \"numexpr\"):\n","        logging.getLogger(noisy).setLevel(logging.WARNING)\n","\n","\n","# ---------------------------------------------------------------------------\n","# IO + Config\n","# ---------------------------------------------------------------------------\n","\n","def ensure_io() -> None:\n","    \"\"\"Ensure Week-3 exists and Week-4 output dirs are present.\"\"\"\n","    if not W3_TRAIN_DIR.is_dir():\n","        raise FileNotFoundError(f\"Missing training data dir: {W3_TRAIN_DIR}\")\n","    for d in (W4_OUT, MODELS_DIR, METRICS_DIR, REPORTS_DIR, TABLES_DIR):\n","        d.mkdir(parents=True, exist_ok=True)\n","\n","\n","def load_config() -> TrainConfig:\n","    \"\"\"\n","    Load config from `week_4/outputs/config.json` if present and merge over defaults.\n","    WHY: explicit config keeps runs reproducible and reviewable.\n","    \"\"\"\n","    cfg_path = W4_OUT / \"config.json\"\n","    cfg = TrainConfig()\n","    if not cfg_path.exists():\n","        return cfg\n","\n","    with cfg_path.open(\"r\", encoding=\"utf-8\") as f:\n","        raw: Dict[str, object] = json.load(f)\n","\n","    # Shallow mapping with validation of known keys\n","    mapping = {\n","        \"target\": \"target\",\n","        \"id_column\": \"id_column\",\n","        \"test_size\": \"test_size\",\n","        \"random_state\": \"random_state\",\n","        \"class_weight\": \"class_weight\",\n","        \"n_jobs\": \"n_jobs\",\n","        \"max_categories\": \"max_categories\",\n","        # nested compat\n","        \"rf\": None,\n","        \"logreg\": None,\n","        \"rf_n_estimators\": \"rf_n_estimators\",\n","        \"rf_max_depth\": \"rf_max_depth\",\n","        \"rf_min_samples_leaf\": \"rf_min_samples_leaf\",\n","        \"logreg_C\": \"logreg_C\",\n","        \"logreg_max_iter\": \"logreg_max_iter\",\n","    }\n","\n","    # Start from defaults\n","    data = cfg.__dict__.copy()\n","\n","    # Flatten possible nested rf/logreg blocks\n","    rf_block = raw.get(\"rf\") if isinstance(raw, dict) else None\n","    logreg_block = raw.get(\"logreg\") if isinstance(raw, dict) else None\n","    if isinstance(rf_block, dict):\n","        raw = {**raw, \"rf_n_estimators\": rf_block.get(\"n_estimators\", data[\"rf_n_estimators\"]),\n","               \"rf_max_depth\": rf_block.get(\"max_depth\", data[\"rf_max_depth\"]),\n","               \"rf_min_samples_leaf\": rf_block.get(\"min_samples_leaf\", data[\"rf_min_samples_leaf\"])}\n","    if isinstance(logreg_block, dict):\n","        raw = {**raw, \"logreg_C\": logreg_block.get(\"C\", data[\"logreg_C\"]),\n","               \"logreg_max_iter\": logreg_block.get(\"max_iter\", data[\"logreg_max_iter\"])}\n","\n","    for k, v in raw.items():\n","        if k in mapping and mapping[k]:\n","            data[mapping[k]] = v\n","\n","    return TrainConfig(**data)\n","\n","\n","def discover_training_sets() -> List[TrainFile]:\n","    \"\"\"Find `train_ready_*.parquet` (WHY: domain and ALL handled uniformly).\"\"\"\n","    files: List[TrainFile] = []\n","    for p in sorted(W3_TRAIN_DIR.glob(\"train_ready_*.parquet\")):\n","        domain = p.stem.replace(\"train_ready_\", \"\", 1)\n","        files.append(TrainFile(domain=domain, path=p))\n","    if not files:\n","        raise FileNotFoundError(f\"No train_ready_*.parquet under {W3_TRAIN_DIR}\")\n","    return files\n","\n","\n","# ---------------------------------------------------------------------------\n","# Preprocessing\n","# ---------------------------------------------------------------------------\n","\n","def split_features(df: pd.DataFrame, target: str, id_column: Optional[str]) -> Tuple[pd.DataFrame, pd.Series]:\n","    \"\"\"Split X/y and drop id column if present.\"\"\"\n","    if target not in df.columns:\n","        raise KeyError(f\"Target column '{target}' not found.\")\n","    y = df[target]\n","    drop_cols: List[str] = [target]\n","    if id_column and id_column in df.columns:\n","        drop_cols.append(id_column)\n","    X = df.drop(columns=drop_cols)\n","    return X, y\n","\n","\n","def drop_constant_columns(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Remove constant columns (WHY: avoid degenerate features and wasted OHE width).\"\"\"\n","    nunique = df.nunique(dropna=False)\n","    to_drop = nunique.index[nunique <= 1].tolist()\n","    return df.drop(columns=to_drop) if to_drop else df\n","\n","\n","def infer_types(X: pd.DataFrame) -> Tuple[List[str], List[str]]:\n","    \"\"\"Infer numeric vs categorical columns.\"\"\"\n","    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n","    cat_cols = [c for c in X.columns if c not in num_cols]\n","    return num_cols, cat_cols\n","\n","\n","def make_ohe(max_categories: int) -> OneHotEncoder:\n","    \"\"\"\n","    Create OneHotEncoder with sklearn version compatibility.\n","    WHY: Colab sklearn versions differ; use sparse_output when available.\n","    \"\"\"\n","    try:\n","        return OneHotEncoder(\n","            handle_unknown=\"ignore\",\n","            sparse_output=False,   # sklearn >=1.2\n","            max_categories=max_categories,\n","        )\n","    except TypeError:\n","        # Fallback for older sklearn\n","        return OneHotEncoder(\n","            handle_unknown=\"ignore\",\n","            sparse=False,\n","            # max_categories not supported in very old versions; ignore if needed\n","        )\n","\n","\n","def build_preprocessor(num_cols: Sequence[str], cat_cols: Sequence[str], max_categories: int) -> ColumnTransformer:\n","    \"\"\"Column-wise preprocessing pipeline.\"\"\"\n","    num_pipe = Pipeline(\n","        steps=[\n","            (\"imputer\", SimpleImputer(strategy=\"median\")),\n","            (\"scaler\", StandardScaler()),\n","        ]\n","    )\n","    cat_pipe = Pipeline(\n","        steps=[\n","            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n","            (\"ohe\", make_ohe(max_categories=max_categories)),\n","        ]\n","    )\n","    return ColumnTransformer(\n","        transformers=[\n","            (\"num\", num_pipe, list(num_cols)),\n","            (\"cat\", cat_pipe, list(cat_cols)),\n","        ],\n","        remainder=\"drop\",\n","    )\n","\n","\n","# ---------------------------------------------------------------------------\n","# Models + Metrics\n","# ---------------------------------------------------------------------------\n","\n","def build_models(pre: ColumnTransformer, cfg: TrainConfig) -> Dict[str, Pipeline]:\n","    \"\"\"Define baseline model pipelines (WHY: strong, fast baselines).\"\"\"\n","    logreg = Pipeline(\n","        steps=[\n","            (\"pre\", pre),\n","            (\"clf\", LogisticRegression(\n","                solver=\"saga\",\n","                penalty=\"l2\",\n","                C=float(cfg.logreg_C),\n","                max_iter=int(cfg.logreg_max_iter),\n","                n_jobs=int(cfg.n_jobs),\n","                class_weight=cfg.class_weight,\n","                random_state=int(cfg.random_state),\n","            )),\n","        ]\n","    )\n","    rf = Pipeline(\n","        steps=[\n","            (\"pre\", pre),\n","            (\"clf\", RandomForestClassifier(\n","                n_estimators=int(cfg.rf_n_estimators),\n","                max_depth=cfg.rf_max_depth,\n","                min_samples_leaf=int(cfg.rf_min_samples_leaf),\n","                n_jobs=int(cfg.n_jobs),\n","                class_weight=cfg.class_weight,\n","                random_state=int(cfg.random_state),\n","            )),\n","        ]\n","    )\n","    return {\"logreg\": logreg, \"rf\": rf}\n","\n","\n","def safe_roc_auc(y_true: pd.Series, y_proba: Optional[np.ndarray]) -> Optional[float]:\n","    \"\"\"ROC-AUC for binary/multiclass (WHY: robust to missing proba).\"\"\"\n","    if y_proba is None:\n","        return None\n","    try:\n","        if y_proba.ndim == 1 or y_proba.shape[1] == 1:\n","            return None\n","        if y_proba.shape[1] == 2:\n","            return float(roc_auc_score(y_true, y_proba[:, 1]))\n","        return float(roc_auc_score(y_true, y_proba, multi_class=\"ovr\", average=\"weighted\"))\n","    except Exception:\n","        return None\n","\n","\n","def evaluate_metrics(y_true: pd.Series, y_pred: np.ndarray, y_proba: Optional[np.ndarray]) -> Dict[str, Optional[float]]:\n","    \"\"\"Compact, stable metric set.\"\"\"\n","    metrics: Dict[str, Optional[float]] = {\n","        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n","        \"precision_weighted\": float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n","        \"recall_weighted\": float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n","        \"f1_weighted\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n","        \"roc_auc\": safe_roc_auc(y_true, y_proba),\n","        \"log_loss\": None,\n","    }\n","    try:\n","        if y_proba is not None:\n","            metrics[\"log_loss\"] = float(log_loss(y_true, y_proba, labels=np.unique(y_true)))\n","    except Exception:\n","        # WHY: older sklearn/proba edge cases; do not fail the run.\n","        pass\n","    return metrics\n","\n","\n","def classification_report_df(y_true: pd.Series, y_pred: np.ndarray) -> pd.DataFrame:\n","    \"\"\"Turn sklearn report into DataFrame for CSV exporting.\"\"\"\n","    rep = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n","    return pd.DataFrame(rep).transpose()\n","\n","\n","# ---------------------------------------------------------------------------\n","# Persistence\n","# ---------------------------------------------------------------------------\n","\n","def persist_artifacts(\n","    domain: str,\n","    model_name: str,\n","    model: Pipeline,\n","    metrics: Dict[str, Optional[float]],\n","    report_df: pd.DataFrame,\n",") -> None:\n","    \"\"\"Persist model + metrics + report (WHY: reproducible artifacts).\"\"\"\n","    joblib.dump(model, MODELS_DIR / f\"{domain}_{model_name}.joblib\")\n","    (METRICS_DIR / f\"{domain}_{model_name}.json\").write_text(\n","        json.dumps(metrics, indent=2), encoding=\"utf-8\"\n","    )\n","    report_df.to_csv(REPORTS_DIR / f\"{domain}_{model_name}_classification_report.csv\", index=True)\n","\n","\n","# ---------------------------------------------------------------------------\n","# Training Loop\n","# ---------------------------------------------------------------------------\n","\n","def train_on_dataframe(df: pd.DataFrame, domain: str, cfg: TrainConfig) -> List[Dict[str, object]]:\n","    \"\"\"Train/evaluate both baselines on a single domain; return leaderboard rows.\"\"\"\n","    df = drop_constant_columns(df.copy())\n","    X, y = split_features(df, cfg.target, cfg.id_column)\n","    num_cols, cat_cols = infer_types(X)\n","    pre = build_preprocessor(num_cols, cat_cols, cfg.max_categories)\n","    models = build_models(pre, cfg)\n","\n","    X_tr, X_te, y_tr, y_te = train_test_split(\n","        X, y,\n","        test_size=cfg.test_size,\n","        random_state=cfg.random_state,\n","        stratify=y,\n","    )\n","\n","    rows: List[Dict[str, object]] = []\n","    for name, pipe in models.items():\n","        pipe.fit(X_tr, y_tr)\n","\n","        y_pred = pipe.predict(X_te)\n","        try:\n","            y_proba = pipe.predict_proba(X_te)\n","        except Exception:\n","            y_proba = None\n","\n","        met = evaluate_metrics(y_te, y_pred, y_proba)\n","        cm = confusion_matrix(y_te, y_pred)\n","        report_df = classification_report_df(y_te, y_pred)\n","\n","        persist_artifacts(domain, name, pipe, {**met, \"n_test\": int(len(y_te))}, report_df)\n","\n","        rows.append(\n","            {\n","                \"domain\": domain,\n","                \"model\": name,\n","                **met,\n","                \"n_train\": int(len(y_tr)),\n","                \"n_test\": int(len(y_te)),\n","                \"n_features_raw\": int(X.shape[1]),\n","                \"n_num\": int(len(num_cols)),\n","                \"n_cat\": int(len(cat_cols)),\n","                \"classes\": \",\".join(map(str, np.unique(y))),\n","                \"confusion_matrix\": \";\".join(\",\".join(map(str, r)) for r in cm),\n","            }\n","        )\n","\n","    return rows\n","\n","\n","# ---------------------------------------------------------------------------\n","# Reporting\n","# ---------------------------------------------------------------------------\n","\n","def write_results_markdown(runs_df: pd.DataFrame, leaderboard: pd.DataFrame) -> None:\n","    \"\"\"Compact summary for human inspection (WHY: quick wins without notebooks).\"\"\"\n","    lines: List[str] = []\n","    lines.append(\"# Week 4 Results\\n\")\n","    lines.append(\"## Leaderboard (best per domain)\\n\")\n","    lines.append(leaderboard.to_markdown(index=False))\n","    lines.append(\"\\n## All Runs\\n\")\n","    keep = [\n","        \"domain\", \"model\", \"f1_weighted\", \"accuracy\",\n","        \"precision_weighted\", \"recall_weighted\", \"roc_auc\", \"log_loss\",\n","        \"n_train\", \"n_test\",\n","    ]\n","    runs_tbl = (\n","        runs_df[keep]\n","        .sort_values([\"domain\", \"f1_weighted\", \"accuracy\"], ascending=[True, False, False])\n","        .to_markdown(index=False)\n","    )\n","    lines.append(runs_tbl)\n","    lines.append(\"\\n---\\nNext: inspect per-model CSV reports under `week_4/outputs/reports/`.\\n\")\n","    RESULTS_MD.write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n","\n","\n","# ---------------------------------------------------------------------------\n","# Orchestration\n","# ---------------------------------------------------------------------------\n","\n","def run() -> None:\n","    \"\"\"End-to-end orchestrator (WHY: single entrypoint for clarity).\"\"\"\n","    setup_logging()\n","    ensure_io()\n","    cfg = load_config()\n","    files = discover_training_sets()\n","\n","    logging.info(\"Discovered %d dataset(s).\", len(files))\n","\n","    rows: List[Dict[str, object]] = []\n","    for tf in files:\n","        logging.info(\"Training on domain '%s' …\", tf.domain)\n","        try:\n","            df = pd.read_parquet(tf.path)\n","            rows.extend(train_on_dataframe(df, tf.domain, cfg))\n","            logging.info(\"Done: %s\", tf.domain)\n","        except Exception as exc:\n","            # WHY: keep the run going if a single domain fails.\n","            logging.exception(\"Failed on domain '%s': %s\", tf.domain, exc)\n","\n","    if not rows:\n","        raise RuntimeError(\"No successful runs; aborting.\")\n","\n","    runs_df = pd.DataFrame(rows)\n","    runs_df.to_csv(TABLES_DIR / \"runs.csv\", index=False)\n","\n","    leaderboard = (\n","        runs_df.sort_values([\"domain\", \"f1_weighted\", \"accuracy\"], ascending=[True, False, False])\n","        .groupby(\"domain\", as_index=False)\n","        .first()\n","    )\n","    leaderboard.to_csv(TABLES_DIR / \"leaderboard.csv\", index=False)\n","\n","    write_results_markdown(runs_df, leaderboard)\n","\n","    logging.info(\"Artifacts:\")\n","    logging.info(\"  models:   %s\", MODELS_DIR)\n","    logging.info(\"  metrics:  %s\", METRICS_DIR)\n","    logging.info(\"  reports:  %s\", REPORTS_DIR)\n","    logging.info(\"  tables:   %s\", TABLES_DIR)\n","    logging.info(\"  summary:  %s\", RESULTS_MD)\n","    logging.info(\"Week 4 complete.\")\n","\n","\n","if __name__ == \"__main__\":\n","    run()\n"],"metadata":{"id":"pVji52eLLSQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2) Read the summary (results.md)\n","print(open(\"/content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_4/outputs/results.md\", \"r\", encoding=\"utf-8\").read())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jWXlnzAaTElZ","executionInfo":{"status":"ok","timestamp":1762350881015,"user_tz":-300,"elapsed":12,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"03187244-47fc-4334-c948-84e08e004a07"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["# Week 4 Results\n","\n","## Leaderboard (best per domain)\n","\n","| domain   | model   |   accuracy |   precision_weighted |   recall_weighted |   f1_weighted |   roc_auc |    log_loss |   n_train |   n_test |   n_features_raw |   n_num |   n_cat | classes   | confusion_matrix    |\n","|:---------|:--------|-----------:|---------------------:|------------------:|--------------:|----------:|------------:|----------:|---------:|-----------------:|--------:|--------:|:----------|:--------------------|\n","| ALL      | logreg  |          1 |                    1 |                 1 |             1 |         1 | 2.44302e-06 |  17491860 |  4372966 |                2 |       0 |       2 | 0,1       | 1708807,0;0,2664159 |\n","| IoT      | logreg  |          1 |                    1 |                 1 |             1 |         1 | 1.3175e-05  |   2884907 |   721227 |                1 |       0 |       1 | 0,1       | 617395,0;0,103832   |\n","| Linux    | logreg  |          1 |                    1 |                 1 |             1 |         1 | 8.51285e-06 |   4684263 |  1171066 |                1 |       0 |       1 | 0,1       | 1008341,0;0,162725  |\n","| Network  | logreg  |          1 |                    1 |                 1 |             1 |         1 | 3.96268e-06 |   9871216 |  2467805 |                1 |       0 |       1 | 0,1       | 73619,0;0,2394186   |\n","| Windows  | logreg  |          1 |                    1 |                 1 |             1 |         1 | 0.000510355 |     51473 |    12869 |                1 |       0 |       1 | 0,1       | 9452,0;0,3417       |\n","\n","## All Runs\n","\n","| domain   | model   |   f1_weighted |   accuracy |   precision_weighted |   recall_weighted |   roc_auc |    log_loss |   n_train |   n_test |\n","|:---------|:--------|--------------:|-----------:|---------------------:|------------------:|----------:|------------:|----------:|---------:|\n","| ALL      | logreg  |             1 |          1 |                    1 |                 1 |         1 | 2.44302e-06 |  17491860 |  4372966 |\n","| ALL      | rf      |             1 |          1 |                    1 |                 1 |         1 | 2.22045e-16 |  17491860 |  4372966 |\n","| IoT      | logreg  |             1 |          1 |                    1 |                 1 |         1 | 1.3175e-05  |   2884907 |   721227 |\n","| IoT      | rf      |             1 |          1 |                    1 |                 1 |         1 | 2.22045e-16 |   2884907 |   721227 |\n","| Linux    | logreg  |             1 |          1 |                    1 |                 1 |         1 | 8.51285e-06 |   4684263 |  1171066 |\n","| Linux    | rf      |             1 |          1 |                    1 |                 1 |         1 | 2.22045e-16 |   4684263 |  1171066 |\n","| Network  | logreg  |             1 |          1 |                    1 |                 1 |         1 | 3.96268e-06 |   9871216 |  2467805 |\n","| Network  | rf      |             1 |          1 |                    1 |                 1 |         1 | 2.22045e-16 |   9871216 |  2467805 |\n","| Windows  | logreg  |             1 |          1 |                    1 |                 1 |         1 | 0.000510355 |     51473 |    12869 |\n","| Windows  | rf      |             1 |          1 |                    1 |                 1 |         1 | 2.22045e-16 |     51473 |    12869 |\n","\n","---\n","Next: inspect per-model CSV reports under `week_4/outputs/reports/`.\n","\n"]}]}]}