{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNPI0tsPsetrM00DXZ5nHC8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzWx7p91jDpL","executionInfo":{"status":"ok","timestamp":1762056076933,"user_tz":-300,"elapsed":34,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"84c3ae41-b890-4c9b-83b6-6788fb79e08e"},"outputs":[{"output_type":"stream","name":"stdout","text":["week_3 root: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3\n","session meta: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/session_meta.json\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-913712712.py:66: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  created_at=datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n"]}],"source":["# /colab_notebooks/week3_step1_bootstrap.ipynb\n","# Week 3 — Step 1: initialize week_3 and validate Week 2 prerequisites\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass, asdict\n","from datetime import datetime\n","from typing import Dict\n","\n","# ----------------------------- Paths ----------------------------- #\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","DATASET_ROOT = os.path.join(PROJECT_ROOT, \"dataset\")\n","\n","W2_OUT = os.path.join(PROJECT_ROOT, \"week_2\", \"outputs\")\n","W3_ROOT = os.path.join(PROJECT_ROOT, \"week_3\")\n","W3_NB   = os.path.join(W3_ROOT, \"notebooks\")\n","W3_OUT  = os.path.join(W3_ROOT, \"outputs\")\n","\n","REQUIRED_W2 = {\n","    \"session_meta\":        os.path.join(W2_OUT, \"session_meta.json\"),\n","    \"eda_files\":           os.path.join(W2_OUT, \"eda_files.csv\"),\n","    \"eda_overview\":        os.path.join(W2_OUT, \"eda_overview.csv\"),\n","    \"column_profile\":      os.path.join(W2_OUT, \"column_profile.csv\"),\n","    \"dtype_summary\":       os.path.join(W2_OUT, \"dtype_summary.csv\"),\n","    \"target_availability\": os.path.join(W2_OUT, \"target_availability.csv\"),\n","    \"validation_report\":   os.path.join(W2_OUT, \"validation_report.json\"),\n","    \"week2_report_md\":     os.path.join(W2_OUT, \"week2_report.md\"),\n","}\n","\n","# ----------------------------- Models ---------------------------- #\n","@dataclass(frozen=True)\n","class SessionMeta:\n","    project_root: str\n","    dataset_root: str\n","    week_2_outputs: Dict[str, str]\n","    week_3: Dict[str, str]\n","    created_at: str\n","    notes: str\n","\n","# ---------------------------- Helpers ---------------------------- #\n","def ensure_drive() -> None:\n","    from google.colab import drive  # type: ignore\n","    if not os.path.ismount(DRIVE_MOUNT_PT):\n","        drive.mount(DRIVE_MOUNT_PT)\n","\n","def ensure_dirs() -> None:\n","    os.makedirs(W3_NB, exist_ok=True)\n","    os.makedirs(W3_OUT, exist_ok=True)\n","\n","def validate_w2() -> Dict[str, str]:\n","    missing = [k for k, p in REQUIRED_W2.items() if not os.path.isfile(p)]\n","    if missing:\n","        raise FileNotFoundError(\"Week 2 prerequisites not found: \" + \", \".join(missing))\n","    return REQUIRED_W2\n","\n","def write_session_meta(resolved: Dict[str, str]) -> str:\n","    meta = SessionMeta(\n","        project_root=PROJECT_ROOT,\n","        dataset_root=DATASET_ROOT,\n","        week_2_outputs=resolved,\n","        week_3={\"root\": W3_ROOT, \"outputs\": W3_OUT, \"notebooks\": W3_NB},\n","        created_at=datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n","        notes=\"Week 3 initialized.\",\n","    )\n","    out_path = os.path.join(W3_OUT, \"session_meta.json\")\n","    with open(out_path, \"w\") as f:\n","        json.dump(asdict(meta), f, indent=2)\n","    return out_path\n","\n","def main() -> None:\n","    ensure_drive()\n","    ensure_dirs()\n","    resolved = validate_w2()\n","    meta_path = write_session_meta(resolved)\n","    print(f\"week_3 root: {W3_ROOT}\")\n","    print(f\"session meta: {meta_path}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","source":["# /colab_notebooks/week3_step2_manifest.ipynb\n","# Week 3 — Step 2: build unified dataset manifest with per-file row estimates\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Tuple\n","\n","import pandas as pd\n","\n","try:\n","    import pyarrow.parquet as pq\n","except Exception:\n","    raise ImportError(\"pyarrow is required for Parquet row counting. Install with: pip install pyarrow\")\n","\n","# ----------------------------- Paths ----------------------------- #\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","W1_OUT = os.path.join(PROJECT_ROOT, \"week_1\", \"outputs\")\n","W3_OUT = os.path.join(PROJECT_ROOT, \"week_3\", \"outputs\")\n","\n","INVENTORY_CSV = os.path.join(W1_OUT, \"data_inventory_paths.csv\")\n","SCHEMA_JSON   = os.path.join(W1_OUT, \"schema_preview.json\")\n","\n","MANIFEST_CSV  = os.path.join(W3_OUT, \"manifest.csv\")\n","\n","os.makedirs(W3_OUT, exist_ok=True)\n","\n","# ----------------------------- Models ---------------------------- #\n","@dataclass(frozen=True)\n","class FileEntry:\n","    domain: str\n","    path: str\n","    extension: str\n","    size_bytes: Optional[int]\n","\n","@dataclass(frozen=True)\n","class Targets:\n","    has_label: bool\n","    has_type: bool\n","    label_col: Optional[str]\n","    type_col: Optional[str]\n","\n","# ----------------------------- Helpers --------------------------- #\n","def _ensure_inputs() -> None:\n","    if not os.path.isfile(INVENTORY_CSV):\n","        raise FileNotFoundError(f\"Required file not found: {INVENTORY_CSV}\")\n","    if not os.path.isfile(SCHEMA_JSON):\n","        raise FileNotFoundError(f\"Required file not found: {SCHEMA_JSON}\")\n","\n","def _read_inventory() -> List[FileEntry]:\n","    df = pd.read_csv(INVENTORY_CSV)\n","    if df.empty:\n","        raise ValueError(\"Inventory contains no entries.\")\n","    # size_bytes might be NaN; coerce to int or None\n","    def _sb(v):\n","        try:\n","            return int(v)\n","        except Exception:\n","            return None\n","    return [\n","        FileEntry(\n","            domain=str(r[\"domain\"]),\n","            path=str(r[\"path\"]),\n","            extension=str(r[\"extension\"]).lower(),\n","            size_bytes=_sb(r.get(\"size_bytes\")),\n","        )\n","        for _, r in df.iterrows()\n","    ]\n","\n","def _read_schema_columns() -> Dict[str, List[str]]:\n","    data = json.load(open(SCHEMA_JSON, \"r\"))\n","    mapping: Dict[str, List[str]] = {}\n","    for rec in data:\n","        p = str(rec.get(\"path\", \"\"))\n","        cols = rec.get(\"columns\", [])\n","        if p and isinstance(cols, list):\n","            mapping[p] = [str(c) for c in cols]\n","    return mapping\n","\n","def _infer_targets(cols: List[str]) -> Targets:\n","    low = {c.lower(): c for c in cols}\n","    label = low.get(\"label\") or low.get(\"labels\") or low.get(\"target\") or low.get(\"class\")\n","    type_ = low.get(\"type\") or low.get(\"attack_cat\")\n","    return Targets(\n","        has_label=label is not None,\n","        has_type=type_ is not None,\n","        label_col=label,\n","        type_col=type_,\n","    )\n","\n","def _count_rows_csv(path: str, chunksize: int = 200_000) -> int:\n","    total = 0\n","    for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n","        total += len(chunk)\n","    return total\n","\n","def _count_rows_parquet(path: str) -> int:\n","    pf = pq.ParquetFile(path)\n","    return sum(pf.metadata.row_group(i).num_rows for i in range(pf.metadata.num_row_groups))\n","\n","def _estimate_rows(entry: FileEntry) -> int:\n","    if entry.extension == \".csv\":\n","        return _count_rows_csv(entry.path)\n","    if entry.extension == \".parquet\":\n","        return _count_rows_parquet(entry.path)\n","    return 0\n","\n","def _fmt_mb(nbytes: Optional[int]) -> float:\n","    if nbytes is None:\n","        return 0.0\n","    return round(nbytes / (1024 * 1024), 3)\n","\n","# ------------------------------ Main ----------------------------- #\n","def main() -> None:\n","    _ensure_inputs()\n","    entries = _read_inventory()\n","    schema_cols = _read_schema_columns()\n","\n","    rows = []\n","    for e in entries:\n","        cols = schema_cols.get(e.path, [])\n","        t = _infer_targets(cols)\n","        try:\n","            est_rows = _estimate_rows(e)\n","        except Exception:\n","            est_rows = 0\n","        rows.append(\n","            {\n","                \"domain\": e.domain,\n","                \"path\": e.path,\n","                \"extension\": e.extension,\n","                \"size_bytes\": e.size_bytes if e.size_bytes is not None else \"\",\n","                \"size_mb\": _fmt_mb(e.size_bytes),\n","                \"est_rows\": int(est_rows),\n","                \"has_label_col\": t.has_label,\n","                \"has_type_col\": t.has_type,\n","                \"label_col\": t.label_col or \"\",\n","                \"type_col\": t.type_col or \"\",\n","            }\n","        )\n","\n","    manifest = pd.DataFrame(rows).sort_values([\"domain\", \"extension\", \"path\"]).reset_index(drop=True)\n","    manifest.to_csv(MANIFEST_CSV, index=False)\n","    print(f\"manifest: {MANIFEST_CSV}\")\n","    # optional glance\n","    by_domain = (\n","        manifest.groupby(\"domain\")\n","        .agg(files=(\"path\", \"count\"),\n","             total_rows=(\"est_rows\", \"sum\"),\n","             total_size_mb=(\"size_mb\", \"sum\"),\n","             files_with_label=(\"has_label_col\", \"sum\"),\n","             files_with_type=(\"has_type_col\", \"sum\"))\n","        .reset_index()\n","    )\n","    print(by_domain.to_string(index=False))\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GKnYzbx1j6MG","executionInfo":{"status":"ok","timestamp":1762056176934,"user_tz":-300,"elapsed":96662,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"5fd74659-0336-4d98-892c-eb71d43b835a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3727237983.py:97: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n","/tmp/ipython-input-3727237983.py:97: DtypeWarning: Columns (2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n","/tmp/ipython-input-3727237983.py:97: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n","/tmp/ipython-input-3727237983.py:97: DtypeWarning: Columns (2,3,4,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n","/tmp/ipython-input-3727237983.py:97: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n","/tmp/ipython-input-3727237983.py:97: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n","/tmp/ipython-input-3727237983.py:97: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n"]},{"output_type":"stream","name":"stdout","text":["manifest: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/manifest.csv\n"," domain  files  total_rows  total_size_mb  files_with_label  files_with_type\n","    IoT      7     3606134        166.995                 7                7\n","  Linux      6     5855329        316.272                 6                6\n","Network     13    12339021       1755.110                13               13\n","Windows      2       64342         59.484                 2                2\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3727237983.py:97: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,47,48,49,50,51,52,59,60,61,65,66,67,71,72,73,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,104,105,106,107,109,112,113,116,127,130,132) have mixed types. Specify dtype option on import or set low_memory=False.\n","  for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n"]}]},{"cell_type":"code","source":["# /colab_notebooks/week3_step3_feature_plan.ipynb\n","# Week 3 — Step 3: derive common feature set + write config\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass, asdict\n","from typing import Dict, List, Set\n","\n","import pandas as pd\n","\n","# ----------------------------- Paths ----------------------------- #\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","W1_OUT = os.path.join(PROJECT_ROOT, \"week_1\", \"outputs\")\n","W2_OUT = os.path.join(PROJECT_ROOT, \"week_2\", \"outputs\")\n","W3_OUT = os.path.join(PROJECT_ROOT, \"week_3\", \"outputs\")\n","\n","SCHEMA_JSON = os.path.join(W1_OUT, \"schema_preview.json\")\n","COLPROF_CSV = os.path.join(W2_OUT, \"column_profile.csv\")\n","\n","FEATURES_CSV = os.path.join(W3_OUT, \"features_common.csv\")\n","CONFIG_JSON  = os.path.join(W3_OUT, \"config.json\")\n","\n","os.makedirs(W3_OUT, exist_ok=True)\n","\n","# ----------------------------- Params ---------------------------- #\n","COVERAGE_THRESHOLD = 80.0   # % of files in a domain that must contain the column\n","RANDOM_SEED = 42\n","MAX_SAMPLE_ROWS = 10_000\n","\n","# ----------------------------- Models ---------------------------- #\n","@dataclass(frozen=True)\n","class FeaturePlan:\n","    coverage_threshold: float\n","    random_seed: int\n","    max_sample_rows: int\n","    targets: Dict[str, List[str]]\n","    drop_patterns: List[str]\n","    features_common_path: str\n","\n","# ---------------------------- Helpers ---------------------------- #\n","def _ensure_inputs() -> None:\n","    if not os.path.isfile(SCHEMA_JSON):\n","        raise FileNotFoundError(f\"Required file not found: {SCHEMA_JSON}\")\n","    if not os.path.isfile(COLPROF_CSV):\n","        raise FileNotFoundError(f\"Required file not found: {COLPROF_CSV}\")\n","\n","def _infer_targets_from_schema() -> Dict[str, List[str]]:\n","    data = json.load(open(SCHEMA_JSON, \"r\"))\n","    # Collect all candidate target names encountered\n","    all_targets: Set[str] = set()\n","    for rec in data:\n","        for c in rec.get(\"candidate_targets\", []) or []:\n","            all_targets.add(str(c))\n","    # Normalize common spellings; keep original names as options\n","    normalized = []\n","    low = {t.lower(): t for t in all_targets}\n","    for key in (\"label\",\"labels\",\"target\",\"class\",\"type\",\"attack_cat\"):\n","        if key in low:\n","            normalized.append(low[key])\n","    # Ensure deduplicated, stable order\n","    targets = sorted(set(normalized), key=str.lower)\n","    return {\"candidate_targets\": targets}\n","\n","def _compute_common_features(colprof: pd.DataFrame, thr: float) -> pd.DataFrame:\n","    # Keep columns that are well-covered within each domain\n","    ok = colprof[colprof[\"pct_coverage\"] >= thr]\n","    # Intersect column names across all domains\n","    by_domain = {d: set(df[\"column\"].astype(str)) for d, df in ok.groupby(\"domain\")}\n","    domains = sorted(by_domain.keys())\n","    common = set.intersection(*(by_domain[d] for d in domains)) if domains else set()\n","    # Build a tidy table with per-domain coverage for the common set\n","    subset = colprof[colprof[\"column\"].isin(common)].copy()\n","    pivot = subset.pivot_table(index=\"column\", columns=\"domain\", values=\"pct_coverage\", aggfunc=\"max\")\n","    pivot = pivot.reset_index().sort_values(\"column\").reset_index(drop=True)\n","    return pivot\n","\n","# ------------------------------ Main ----------------------------- #\n","def main() -> None:\n","    _ensure_inputs()\n","    colprof = pd.read_csv(COLPROF_CSV)\n","    if colprof.empty:\n","        raise ValueError(\"column_profile.csv is empty.\")\n","\n","    features_tbl = _compute_common_features(colprof, COVERAGE_THRESHOLD)\n","    features_tbl.to_csv(FEATURES_CSV, index=False)\n","\n","    targets = _infer_targets_from_schema()\n","\n","    drop_patterns = [\n","        r\"^ts$\",\n","        r\"^date$\",\n","        r\"^time$\",\n","        r\"^timestamp$\",\n","        r\"(^|_)id$\",\n","        r\".*ip$\",\n","        r\".*uri$\",\n","        r\".*user[_-]?agent.*\",\n","        r\"^dns_.*\",\n","    ]\n","\n","    plan = FeaturePlan(\n","        coverage_threshold=COVERAGE_THRESHOLD,\n","        random_seed=RANDOM_SEED,\n","        max_sample_rows=MAX_SAMPLE_ROWS,\n","        targets=targets,\n","        drop_patterns=drop_patterns,\n","        features_common_path=FEATURES_CSV,\n","    )\n","    with open(CONFIG_JSON, \"w\") as f:\n","        json.dump(asdict(plan), f, indent=2)\n","\n","    print(f\"features_common: {FEATURES_CSV}\")\n","    print(f\"config:          {CONFIG_JSON}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OM2Tq0HAmZd3","executionInfo":{"status":"ok","timestamp":1762053968948,"user_tz":-300,"elapsed":1099,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"1476ce56-c5cd-44da-b606-166cdf765760"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["features_common: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/features_common.csv\n","config:          /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/config.json\n"]}]},{"cell_type":"code","source":["# /colab_notebooks/week3_step4_materialize_training_data.ipynb\n","# Week 3 — Step 4: build per-domain & ALL training-ready Parquet from common features\n","\n","from __future__ import annotations\n","\n","import os\n","import re\n","import json\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Tuple\n","\n","import pandas as pd\n","\n","try:\n","    import pyarrow as pa\n","    import pyarrow.parquet as pq\n","except Exception:\n","    raise ImportError(\"pyarrow is required. Install with: pip install pyarrow\")\n","\n","# ----------------------------- Paths ----------------------------- #\n","\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","W1_OUT = os.path.join(PROJECT_ROOT, \"week_1\", \"outputs\")\n","W2_OUT = os.path.join(PROJECT_ROOT, \"week_2\", \"outputs\")\n","W3_OUT = os.path.join(PROJECT_ROOT, \"week_3\", \"outputs\")\n","\n","INVENTORY_CSV = os.path.join(W1_OUT, \"data_inventory_paths.csv\")\n","SCHEMA_JSON   = os.path.join(W1_OUT, \"schema_preview.json\")\n","FEATURES_CSV  = os.path.join(W3_OUT, \"features_common.csv\")\n","CONFIG_JSON   = os.path.join(W3_OUT, \"config.json\")\n","\n","OUT_DIR       = os.path.join(W3_OUT, \"training_data\")\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","# ----------------------------- Params ---------------------------- #\n","\n","CSV_CHUNKSIZE = 200_000  # controlled memory for large CSVs\n","\n","# ----------------------------- Models ---------------------------- #\n","\n","@dataclass(frozen=True)\n","class FileEntry:\n","    domain: str\n","    path: str\n","    extension: str\n","\n","@dataclass(frozen=True)\n","class TargetCols:\n","    label_src: Optional[str]\n","    type_src: Optional[str]\n","\n","# ----------------------------- Helpers --------------------------- #\n","\n","def _ensure_inputs() -> None:\n","    for p in [INVENTORY_CSV, SCHEMA_JSON, FEATURES_CSV, CONFIG_JSON]:\n","        if not os.path.isfile(p):\n","            raise FileNotFoundError(f\"Required file not found: {p}\")\n","\n","def _read_inventory() -> List[FileEntry]:\n","    df = pd.read_csv(INVENTORY_CSV)\n","    if df.empty:\n","        raise ValueError(\"Inventory contains no entries.\")\n","    return [FileEntry(str(r[\"domain\"]), str(r[\"path\"]), str(r[\"extension\"]).lower()) for _, r in df.iterrows()]\n","\n","def _read_schema_map() -> Dict[str, List[str]]:\n","    data = json.load(open(SCHEMA_JSON, \"r\"))\n","    m: Dict[str, List[str]] = {}\n","    for rec in data:\n","        p = str(rec.get(\"path\", \"\"))\n","        cols = rec.get(\"columns\", [])\n","        if p and isinstance(cols, list):\n","            m[p] = [str(c) for c in cols]\n","    return m\n","\n","def _read_config() -> Dict:\n","    return json.load(open(CONFIG_JSON, \"r\"))\n","\n","def _load_common_features() -> List[str]:\n","    df = pd.read_csv(FEATURES_CSV)\n","    # expects a \"column\" field (from Week 3 Step 3)\n","    if \"column\" not in df.columns:\n","        raise ValueError(\"features_common.csv must contain a 'column' field.\")\n","    return [str(c) for c in df[\"column\"].tolist()]\n","\n","def _compile_drops(patterns: List[str]) -> List[re.Pattern]:\n","    return [re.compile(p, re.I) for p in patterns]\n","\n","def _should_drop(name: str, drops: List[re.Pattern]) -> bool:\n","    for pat in drops:\n","        if pat.search(name):\n","            return True\n","    return False\n","\n","def _find_targets(cols: List[str]) -> TargetCols:\n","    low = {c.lower(): c for c in cols}\n","    label = low.get(\"label\") or low.get(\"labels\") or low.get(\"target\") or low.get(\"class\")\n","    typ   = low.get(\"type\") or low.get(\"attack_cat\")\n","    return TargetCols(label_src=label, type_src=typ)\n","\n","def _normalize_targets(df: pd.DataFrame, t: TargetCols) -> pd.DataFrame:\n","    # Rename present target columns to standardized 'label' / 'type'\n","    if t.label_src and t.label_src in df.columns and \"label\" not in df.columns:\n","        df = df.rename(columns={t.label_src: \"label\"})\n","    if t.type_src and t.type_src in df.columns and \"type\" not in df.columns:\n","        df = df.rename(columns={t.type_src: \"type\"})\n","    return df\n","\n","def _ensure_columns(df: pd.DataFrame, ordered_cols: List[str]) -> pd.DataFrame:\n","    # Add missing columns as NA, then reorder\n","    missing = [c for c in ordered_cols if c not in df.columns]\n","    for c in missing:\n","        df[c] = pd.NA\n","    return df[ordered_cols]\n","\n","def _to_arrow_table(df: pd.DataFrame) -> pa.Table:\n","    # Keep Arrow schema stable across chunks\n","    return pa.Table.from_pandas(df, preserve_index=False)\n","\n","# ----------------------------- Core I/O --------------------------- #\n","\n","class DomainWriters:\n","    \"\"\"Manages per-domain Parquet writers and a combined 'ALL' writer.\"\"\"\n","    def __init__(self, out_dir: str, desired_cols: List[str]) -> None:\n","        self.out_dir = out_dir\n","        self.desired_cols = desired_cols\n","        self.domain_writers: Dict[str, pq.ParquetWriter] = {}\n","        self.all_writer: Optional[pq.ParquetWriter] = None\n","        self.schemas: Dict[str, pa.Schema] = {}\n","        self.all_schema: Optional[pa.Schema] = None\n","        self.rows_written: Dict[str, int] = {}\n","        self.files_processed: Dict[str, int] = {}\n","\n","    def _path_for(self, domain: str) -> str:\n","        return os.path.join(self.out_dir, f\"train_ready_{domain}.parquet\")\n","\n","    def _all_path(self) -> str:\n","        return os.path.join(self.out_dir, \"train_ready_ALL.parquet\")\n","\n","    def write(self, domain: str, batch_df: pd.DataFrame) -> None:\n","        # Track counts\n","        self.rows_written[domain] = self.rows_written.get(domain, 0) + len(batch_df)\n","        # Domain writer\n","        tbl = _to_arrow_table(batch_df)\n","        if domain not in self.domain_writers:\n","            self.schemas[domain] = tbl.schema\n","            self.domain_writers[domain] = pq.ParquetWriter(self._path_for(domain), self.schemas[domain], compression=\"snappy\")\n","            self.files_processed[domain] = 0\n","        self.domain_writers[domain].write_table(tbl)\n","\n","        # ALL writer (add domain col)\n","        if \"domain\" not in batch_df.columns:\n","            batch_df = batch_df.assign(domain=domain)\n","            tbl_all = _to_arrow_table(batch_df)\n","        else:\n","            tbl_all = _to_arrow_table(batch_df)\n","\n","        if self.all_writer is None:\n","            self.all_schema = tbl_all.schema\n","            self.all_writer = pq.ParquetWriter(self._all_path(), self.all_schema, compression=\"snappy\")\n","        self.all_writer.write_table(tbl_all)\n","\n","    def bump_file_counter(self, domain: str) -> None:\n","        self.files_processed[domain] = self.files_processed.get(domain, 0) + 1\n","\n","    def close(self) -> None:\n","        for w in self.domain_writers.values():\n","            w.close()\n","        if self.all_writer is not None:\n","            self.all_writer.close()\n","\n","# ------------------------------ Main ----------------------------- #\n","\n","def main() -> None:\n","    _ensure_inputs()\n","\n","    cfg = _read_config()\n","    common_cols = _load_common_features()\n","    drops = _compile_drops(cfg.get(\"drop_patterns\", []))\n","\n","    inv = _read_inventory()\n","    schema_map = _read_schema_map()\n","\n","    # Desired final columns: common features + standardized targets ('label','type')\n","    desired_cols = [c for c in common_cols if not _should_drop(c, drops)]\n","    desired_cols += [\"label\", \"type\"]  # may be absent in some files; we add as NA\n","    desired_cols = list(dict.fromkeys(desired_cols))  # stable dedupe\n","\n","    writers = DomainWriters(OUT_DIR, desired_cols)\n","    report_rows: List[Dict[str, object]] = []\n","\n","    for e in inv:\n","        cols = schema_map.get(e.path, [])\n","        t = _find_targets(cols)\n","        has_any = bool(t.label_src or t.type_src)\n","\n","        # Determine read columns for this file: common∩file + present targets\n","        file_cols = set(cols)\n","        read_cols = [c for c in common_cols if c in file_cols and not _should_drop(c, drops)]\n","        if t.label_src and t.label_src in file_cols:\n","            read_cols.append(t.label_src)\n","        if t.type_src and t.type_src in file_cols:\n","            read_cols.append(t.type_src)\n","        read_cols = list(dict.fromkeys(read_cols))\n","\n","        total_rows = 0\n","        written_rows = 0\n","\n","        try:\n","            if e.extension == \".csv\":\n","                if read_cols:\n","                    for chunk in pd.read_csv(e.path, usecols=read_cols, chunksize=CSV_CHUNKSIZE, low_memory=True):\n","                        total_rows += len(chunk)\n","                        chunk = _normalize_targets(chunk, t)\n","                        chunk = _ensure_columns(chunk, writers.desired_cols)\n","                        writers.write(e.domain, chunk)\n","                        written_rows += len(chunk)\n","                else:\n","                    # no selected columns; still count rows\n","                    for chunk in pd.read_csv(e.path, usecols=None, chunksize=CSV_CHUNKSIZE, low_memory=True):\n","                        total_rows += len(chunk)\n","            elif e.extension == \".parquet\":\n","                if read_cols:\n","                    pf = pq.ParquetFile(e.path)\n","                    for rg in range(pf.metadata.num_row_groups):\n","                        tbl = pf.read_row_group(rg, columns=read_cols)\n","                        df = tbl.to_pandas()\n","                        total_rows += len(df)\n","                        df = _normalize_targets(df, t)\n","                        df = _ensure_columns(df, writers.desired_cols)\n","                        writers.write(e.domain, df)\n","                        written_rows += len(df)\n","                else:\n","                    pf = pq.ParquetFile(e.path)\n","                    total_rows = sum(pf.metadata.row_group(i).num_rows for i in range(pf.metadata.num_row_groups))\n","        except Exception:\n","            # skip problematic file silently, but record in report\n","            total_rows = total_rows or 0\n","            written_rows = written_rows or 0\n","\n","        if written_rows > 0:\n","            writers.bump_file_counter(e.domain)\n","\n","        report_rows.append({\n","            \"domain\": e.domain,\n","            \"path\": e.path,\n","            \"extension\": e.extension,\n","            \"had_any_target_col\": has_any,\n","            \"selected_cols\": \",\".join(read_cols),\n","            \"rows_total_seen\": int(total_rows),\n","            \"rows_written\": int(written_rows),\n","        })\n","\n","    writers.close()\n","\n","    # Write a compact build report\n","    build_report = pd.DataFrame(report_rows).sort_values([\"domain\", \"rows_written\"], ascending=[True, False])\n","    build_report.to_csv(os.path.join(OUT_DIR, \"build_report.csv\"), index=False)\n","\n","    # Domain summary\n","    dom_summary = (\n","        build_report.groupby(\"domain\")\n","        .agg(files_processed=(\"path\", \"count\"),\n","             files_with_rows=(\"rows_written\", lambda s: int((s > 0).sum())),\n","             rows_total_seen=(\"rows_total_seen\", \"sum\"),\n","             rows_written=(\"rows_written\", \"sum\"))\n","        .reset_index()\n","    )\n","    dom_summary.to_csv(os.path.join(OUT_DIR, \"build_summary.csv\"), index=False)\n","\n","    print(os.path.join(OUT_DIR, \"train_ready_ALL.parquet\"))\n","    for dom in sorted(dom_summary[\"domain\"].unique()):\n","        print(os.path.join(OUT_DIR, f\"train_ready_{dom}.parquet\"))\n","    print(os.path.join(OUT_DIR, \"build_summary.csv\"))\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M1L8q2t1n6bs","executionInfo":{"status":"ok","timestamp":1762054406443,"user_tz":-300,"elapsed":49755,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"c7aa0b7e-3899-4054-fd77-ccf53ba84150"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/train_ready_ALL.parquet\n","/content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/train_ready_IoT.parquet\n","/content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/train_ready_Linux.parquet\n","/content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/train_ready_Network.parquet\n","/content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/train_ready_Windows.parquet\n","/content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/build_summary.csv\n"]}]},{"cell_type":"code","source":["# /colab_notebooks/week3_step5_validate_training_data.ipynb\n","# Week 3 — Step 5: validate materialized Parquet datasets\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass\n","from typing import Dict, List, Tuple\n","\n","import pandas as pd\n","import pyarrow.parquet as pq\n","\n","# ----------------------------- Paths ----------------------------- #\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","W3_OUT        = os.path.join(PROJECT_ROOT, \"week_3\", \"outputs\")\n","TRAIN_DIR     = os.path.join(W3_OUT, \"training_data\")\n","FEATURES_CSV  = os.path.join(W3_OUT, \"features_common.csv\")\n","CONFIG_JSON   = os.path.join(W3_OUT, \"config.json\")\n","\n","SUMMARY_CSV   = os.path.join(TRAIN_DIR, \"validation_summary.csv\")\n","DETAILS_CSV   = os.path.join(TRAIN_DIR, \"validation_details.csv\")\n","\n","# ---------------------------- Helpers ---------------------------- #\n","def _ensure_inputs() -> None:\n","    missing = []\n","    for p in [FEATURES_CSV, CONFIG_JSON]:\n","        if not os.path.isfile(p):\n","            missing.append(p)\n","    for fname in [\"train_ready_ALL.parquet\"]:\n","        if not os.path.isfile(os.path.join(TRAIN_DIR, fname)):\n","            missing.append(os.path.join(TRAIN_DIR, fname))\n","    if missing:\n","        raise FileNotFoundError(\"Missing required artifact(s): \" + \", \".join(missing))\n","\n","def _load_expected_columns() -> List[str]:\n","    feats = pd.read_csv(FEATURES_CSV)\n","    if \"column\" not in feats.columns:\n","        raise ValueError(\"features_common.csv must have a 'column' column.\")\n","    exp = list(dict.fromkeys(feats[\"column\"].astype(str).tolist()))\n","    exp += [\"label\", \"type\"]  # standardized targets if present\n","    return exp\n","\n","def _list_parquet_targets() -> Dict[str, str]:\n","    files = {}\n","    for dom in [\"Windows\", \"Linux\", \"Network\", \"IoT\"]:\n","        p = os.path.join(TRAIN_DIR, f\"train_ready_{dom}.parquet\")\n","        if os.path.isfile(p):\n","            files[dom] = p\n","    files[\"ALL\"] = os.path.join(TRAIN_DIR, \"train_ready_ALL.parquet\")\n","    return files\n","\n","def _pq_schema_cols(path: str) -> List[str]:\n","    pf = pq.ParquetFile(path)\n","    return [pf.schema_arrow.names[i] for i in range(len(pf.schema_arrow.names))]\n","\n","def _pq_rowcount(path: str) -> int:\n","    pf = pq.ParquetFile(path)\n","    return sum(pf.metadata.row_group(i).num_rows for i in range(pf.metadata.num_row_groups))\n","\n","def _value_counts_parquet(path: str, col: str, max_unique: int = 15) -> Tuple[int, Dict[str, int]]:\n","    pf = pq.ParquetFile(path)\n","    counts: Dict[str, int] = {}\n","    total = 0\n","    for i in range(pf.metadata.num_row_groups):\n","        tbl = pf.read_row_group(i, columns=[col]) if col in pf.schema_arrow.names else None\n","        if tbl is None:\n","            continue\n","        s = tbl.to_pandas()[col]\n","        total += s.notna().sum()\n","        vc = s.dropna().astype(str).value_counts()\n","        for k, v in vc.items():\n","            counts[k] = counts.get(k, 0) + int(v)\n","    # keep top max_unique\n","    counts = dict(sorted(counts.items(), key=lambda kv: kv[1], reverse=True)[:max_unique])\n","    return total, counts\n","\n","# ------------------------------ Main ----------------------------- #\n","def main() -> None:\n","    _ensure_inputs()\n","    expected_cols = _load_expected_columns()\n","    targets = {\"label\", \"type\"}\n","\n","    files = _list_parquet_targets()\n","    rows_summary = []\n","    rows_details = []\n","\n","    for dom, path in files.items():\n","        if not os.path.isfile(path):\n","            continue\n","\n","        cols = _pq_schema_cols(path)\n","        nrows = _pq_rowcount(path)\n","\n","        missing_expected = [c for c in expected_cols if c not in cols]\n","        unexpected = [c for c in cols if c not in expected_cols + [\"domain\"]]  # 'domain' may appear in ALL\n","\n","        has_label = \"label\" in cols\n","        has_type  = \"type\" in cols\n","\n","        label_nonnull = type_nonnull = 0\n","        label_vc = {}\n","        type_vc = {}\n","\n","        if has_label:\n","            label_nonnull, label_vc = _value_counts_parquet(path, \"label\")\n","        if has_type:\n","            type_nonnull, type_vc = _value_counts_parquet(path, \"type\")\n","\n","        rows_summary.append({\n","            \"dataset\": dom,\n","            \"path\": path,\n","            \"rows\": nrows,\n","            \"ncols\": len(cols),\n","            \"has_label\": has_label,\n","            \"has_type\": has_type,\n","            \"label_nonnull\": label_nonnull,\n","            \"type_nonnull\": type_nonnull,\n","            \"missing_expected_cols\": len(missing_expected),\n","            \"unexpected_cols\": len(unexpected),\n","        })\n","\n","        rows_details.append({\n","            \"dataset\": dom,\n","            \"path\": path,\n","            \"columns\": \",\".join(cols),\n","            \"missing_expected_cols\": \",\".join(missing_expected),\n","            \"unexpected_cols\": \",\".join(unexpected),\n","            \"label_top_values\": json.dumps(label_vc),\n","            \"type_top_values\": json.dumps(type_vc),\n","        })\n","\n","    pd.DataFrame(rows_summary).sort_values(\"dataset\").to_csv(SUMMARY_CSV, index=False)\n","    pd.DataFrame(rows_details).sort_values(\"dataset\").to_csv(DETAILS_CSV, index=False)\n","\n","    print(f\"validation_summary: {SUMMARY_CSV}\")\n","    print(f\"validation_details: {DETAILS_CSV}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n","# /colab_notebooks/week3_step5_validate_training_data.ipynb\n","# Week 3 — Step 5: validate materialized Parquet datasets\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass\n","from typing import Dict, List, Tuple\n","\n","import pandas as pd\n","import pyarrow.parquet as pq\n","\n","# ----------------------------- Paths ----------------------------- #\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","W3_OUT        = os.path.join(PROJECT_ROOT, \"week_3\", \"outputs\")\n","TRAIN_DIR     = os.path.join(W3_OUT, \"training_data\")\n","FEATURES_CSV  = os.path.join(W3_OUT, \"features_common.csv\")\n","CONFIG_JSON   = os.path.join(W3_OUT, \"config.json\")\n","\n","SUMMARY_CSV   = os.path.join(TRAIN_DIR, \"validation_summary.csv\")\n","DETAILS_CSV   = os.path.join(TRAIN_DIR, \"validation_details.csv\")\n","\n","# ---------------------------- Helpers ---------------------------- #\n","def _ensure_inputs() -> None:\n","    missing = []\n","    for p in [FEATURES_CSV, CONFIG_JSON]:\n","        if not os.path.isfile(p):\n","            missing.append(p)\n","    for fname in [\"train_ready_ALL.parquet\"]:\n","        if not os.path.isfile(os.path.join(TRAIN_DIR, fname)):\n","            missing.append(os.path.join(TRAIN_DIR, fname))\n","    if missing:\n","        raise FileNotFoundError(\"Missing required artifact(s): \" + \", \".join(missing))\n","\n","def _load_expected_columns() -> List[str]:\n","    feats = pd.read_csv(FEATURES_CSV)\n","    if \"column\" not in feats.columns:\n","        raise ValueError(\"features_common.csv must have a 'column' column.\")\n","    exp = list(dict.fromkeys(feats[\"column\"].astype(str).tolist()))\n","    exp += [\"label\", \"type\"]  # standardized targets if present\n","    return exp\n","\n","def _list_parquet_targets() -> Dict[str, str]:\n","    files = {}\n","    for dom in [\"Windows\", \"Linux\", \"Network\", \"IoT\"]:\n","        p = os.path.join(TRAIN_DIR, f\"train_ready_{dom}.parquet\")\n","        if os.path.isfile(p):\n","            files[dom] = p\n","    files[\"ALL\"] = os.path.join(TRAIN_DIR, \"train_ready_ALL.parquet\")\n","    return files\n","\n","def _pq_schema_cols(path: str) -> List[str]:\n","    pf = pq.ParquetFile(path)\n","    return [pf.schema_arrow.names[i] for i in range(len(pf.schema_arrow.names))]\n","\n","def _pq_rowcount(path: str) -> int:\n","    pf = pq.ParquetFile(path)\n","    return sum(pf.metadata.row_group(i).num_rows for i in range(pf.metadata.num_row_groups))\n","\n","def _value_counts_parquet(path: str, col: str, max_unique: int = 15) -> Tuple[int, Dict[str, int]]:\n","    pf = pq.ParquetFile(path)\n","    counts: Dict[str, int] = {}\n","    total = 0\n","    for i in range(pf.metadata.num_row_groups):\n","        tbl = pf.read_row_group(i, columns=[col]) if col in pf.schema_arrow.names else None\n","        if tbl is None:\n","            continue\n","        s = tbl.to_pandas()[col]\n","        total += s.notna().sum()\n","        vc = s.dropna().astype(str).value_counts()\n","        for k, v in vc.items():\n","            counts[k] = counts.get(k, 0) + int(v)\n","    # keep top max_unique\n","    counts = dict(sorted(counts.items(), key=lambda kv: kv[1], reverse=True)[:max_unique])\n","    return total, counts\n","\n","# ------------------------------ Main ----------------------------- #\n","def main() -> None:\n","    _ensure_inputs()\n","    expected_cols = _load_expected_columns()\n","    targets = {\"label\", \"type\"}\n","\n","    files = _list_parquet_targets()\n","    rows_summary = []\n","    rows_details = []\n","\n","    for dom, path in files.items():\n","        if not os.path.isfile(path):\n","            continue\n","\n","        cols = _pq_schema_cols(path)\n","        nrows = _pq_rowcount(path)\n","\n","        missing_expected = [c for c in expected_cols if c not in cols]\n","        unexpected = [c for c in cols if c not in expected_cols + [\"domain\"]]  # 'domain' may appear in ALL\n","\n","        has_label = \"label\" in cols\n","        has_type  = \"type\" in cols\n","\n","        label_nonnull = type_nonnull = 0\n","        label_vc = {}\n","        type_vc = {}\n","\n","        if has_label:\n","            label_nonnull, label_vc = _value_counts_parquet(path, \"label\")\n","        if has_type:\n","            type_nonnull, type_vc = _value_counts_parquet(path, \"type\")\n","\n","        rows_summary.append({\n","            \"dataset\": dom,\n","            \"path\": path,\n","            \"rows\": nrows,\n","            \"ncols\": len(cols),\n","            \"has_label\": has_label,\n","            \"has_type\": has_type,\n","            \"label_nonnull\": label_nonnull,\n","            \"type_nonnull\": type_nonnull,\n","            \"missing_expected_cols\": len(missing_expected),\n","            \"unexpected_cols\": len(unexpected),\n","        })\n","\n","        rows_details.append({\n","            \"dataset\": dom,\n","            \"path\": path,\n","            \"columns\": \",\".join(cols),\n","            \"missing_expected_cols\": \",\".join(missing_expected),\n","            \"unexpected_cols\": \",\".join(unexpected),\n","            \"label_top_values\": json.dumps(label_vc),\n","            \"type_top_values\": json.dumps(type_vc),\n","        })\n","\n","    pd.DataFrame(rows_summary).sort_values(\"dataset\").to_csv(SUMMARY_CSV, index=False)\n","    pd.DataFrame(rows_details).sort_values(\"dataset\").to_csv(DETAILS_CSV, index=False)\n","\n","    print(f\"validation_summary: {SUMMARY_CSV}\")\n","    print(f\"validation_details: {DETAILS_CSV}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n","# /colab_notebooks/week3_step5_validate_training_data.ipynb\n","# Week 3 — Step 5: validate materialized Parquet datasets\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass\n","from typing import Dict, List, Tuple\n","\n","import pandas as pd\n","import pyarrow.parquet as pq\n","\n","# ----------------------------- Paths ----------------------------- #\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","W3_OUT        = os.path.join(PROJECT_ROOT, \"week_3\", \"outputs\")\n","TRAIN_DIR     = os.path.join(W3_OUT, \"training_data\")\n","FEATURES_CSV  = os.path.join(W3_OUT, \"features_common.csv\")\n","CONFIG_JSON   = os.path.join(W3_OUT, \"config.json\")\n","\n","SUMMARY_CSV   = os.path.join(TRAIN_DIR, \"validation_summary.csv\")\n","DETAILS_CSV   = os.path.join(TRAIN_DIR, \"validation_details.csv\")\n","\n","# ---------------------------- Helpers ---------------------------- #\n","def _ensure_inputs() -> None:\n","    missing = []\n","    for p in [FEATURES_CSV, CONFIG_JSON]:\n","        if not os.path.isfile(p):\n","            missing.append(p)\n","    for fname in [\"train_ready_ALL.parquet\"]:\n","        if not os.path.isfile(os.path.join(TRAIN_DIR, fname)):\n","            missing.append(os.path.join(TRAIN_DIR, fname))\n","    if missing:\n","        raise FileNotFoundError(\"Missing required artifact(s): \" + \", \".join(missing))\n","\n","def _load_expected_columns() -> List[str]:\n","    feats = pd.read_csv(FEATURES_CSV)\n","    if \"column\" not in feats.columns:\n","        raise ValueError(\"features_common.csv must have a 'column' column.\")\n","    exp = list(dict.fromkeys(feats[\"column\"].astype(str).tolist()))\n","    exp += [\"label\", \"type\"]  # standardized targets if present\n","    return exp\n","\n","def _list_parquet_targets() -> Dict[str, str]:\n","    files = {}\n","    for dom in [\"Windows\", \"Linux\", \"Network\", \"IoT\"]:\n","        p = os.path.join(TRAIN_DIR, f\"train_ready_{dom}.parquet\")\n","        if os.path.isfile(p):\n","            files[dom] = p\n","    files[\"ALL\"] = os.path.join(TRAIN_DIR, \"train_ready_ALL.parquet\")\n","    return files\n","\n","def _pq_schema_cols(path: str) -> List[str]:\n","    pf = pq.ParquetFile(path)\n","    return [pf.schema_arrow.names[i] for i in range(len(pf.schema_arrow.names))]\n","\n","def _pq_rowcount(path: str) -> int:\n","    pf = pq.ParquetFile(path)\n","    return sum(pf.metadata.row_group(i).num_rows for i in range(pf.metadata.num_row_groups))\n","\n","def _value_counts_parquet(path: str, col: str, max_unique: int = 15) -> Tuple[int, Dict[str, int]]:\n","    pf = pq.ParquetFile(path)\n","    counts: Dict[str, int] = {}\n","    total = 0\n","    for i in range(pf.metadata.num_row_groups):\n","        tbl = pf.read_row_group(i, columns=[col]) if col in pf.schema_arrow.names else None\n","        if tbl is None:\n","            continue\n","        s = tbl.to_pandas()[col]\n","        total += s.notna().sum()\n","        vc = s.dropna().astype(str).value_counts()\n","        for k, v in vc.items():\n","            counts[k] = counts.get(k, 0) + int(v)\n","    # keep top max_unique\n","    counts = dict(sorted(counts.items(), key=lambda kv: kv[1], reverse=True)[:max_unique])\n","    return total, counts\n","\n","# ------------------------------ Main ----------------------------- #\n","def main() -> None:\n","    _ensure_inputs()\n","    expected_cols = _load_expected_columns()\n","    targets = {\"label\", \"type\"}\n","\n","    files = _list_parquet_targets()\n","    rows_summary = []\n","    rows_details = []\n","\n","    for dom, path in files.items():\n","        if not os.path.isfile(path):\n","            continue\n","\n","        cols = _pq_schema_cols(path)\n","        nrows = _pq_rowcount(path)\n","\n","        missing_expected = [c for c in expected_cols if c not in cols]\n","        unexpected = [c for c in cols if c not in expected_cols + [\"domain\"]]  # 'domain' may appear in ALL\n","\n","        has_label = \"label\" in cols\n","        has_type  = \"type\" in cols\n","\n","        label_nonnull = type_nonnull = 0\n","        label_vc = {}\n","        type_vc = {}\n","\n","        if has_label:\n","            label_nonnull, label_vc = _value_counts_parquet(path, \"label\")\n","        if has_type:\n","            type_nonnull, type_vc = _value_counts_parquet(path, \"type\")\n","\n","        rows_summary.append({\n","            \"dataset\": dom,\n","            \"path\": path,\n","            \"rows\": nrows,\n","            \"ncols\": len(cols),\n","            \"has_label\": has_label,\n","            \"has_type\": has_type,\n","            \"label_nonnull\": label_nonnull,\n","            \"type_nonnull\": type_nonnull,\n","            \"missing_expected_cols\": len(missing_expected),\n","            \"unexpected_cols\": len(unexpected),\n","        })\n","\n","        rows_details.append({\n","            \"dataset\": dom,\n","            \"path\": path,\n","            \"columns\": \",\".join(cols),\n","            \"missing_expected_cols\": \",\".join(missing_expected),\n","            \"unexpected_cols\": \",\".join(unexpected),\n","            \"label_top_values\": json.dumps(label_vc),\n","            \"type_top_values\": json.dumps(type_vc),\n","        })\n","\n","    pd.DataFrame(rows_summary).sort_values(\"dataset\").to_csv(SUMMARY_CSV, index=False)\n","    pd.DataFrame(rows_details).sort_values(\"dataset\").to_csv(DETAILS_CSV, index=False)\n","\n","    print(f\"validation_summary: {SUMMARY_CSV}\")\n","    print(f\"validation_details: {DETAILS_CSV}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n","# /colab_notebooks/week3_step5_validate_training_data.ipynb\n","# Week 3 — Step 5: validate materialized Parquet datasets\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass\n","from typing import Dict, List, Tuple\n","\n","import pandas as pd\n","import pyarrow.parquet as pq\n","\n","# ----------------------------- Paths ----------------------------- #\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","W3_OUT        = os.path.join(PROJECT_ROOT, \"week_3\", \"outputs\")\n","TRAIN_DIR     = os.path.join(W3_OUT, \"training_data\")\n","FEATURES_CSV  = os.path.join(W3_OUT, \"features_common.csv\")\n","CONFIG_JSON   = os.path.join(W3_OUT, \"config.json\")\n","\n","SUMMARY_CSV   = os.path.join(TRAIN_DIR, \"validation_summary.csv\")\n","DETAILS_CSV   = os.path.join(TRAIN_DIR, \"validation_details.csv\")\n","\n","# ---------------------------- Helpers ---------------------------- #\n","def _ensure_inputs() -> None:\n","    missing = []\n","    for p in [FEATURES_CSV, CONFIG_JSON]:\n","        if not os.path.isfile(p):\n","            missing.append(p)\n","    for fname in [\"train_ready_ALL.parquet\"]:\n","        if not os.path.isfile(os.path.join(TRAIN_DIR, fname)):\n","            missing.append(os.path.join(TRAIN_DIR, fname))\n","    if missing:\n","        raise FileNotFoundError(\"Missing required artifact(s): \" + \", \".join(missing))\n","\n","def _load_expected_columns() -> List[str]:\n","    feats = pd.read_csv(FEATURES_CSV)\n","    if \"column\" not in feats.columns:\n","        raise ValueError(\"features_common.csv must have a 'column' column.\")\n","    exp = list(dict.fromkeys(feats[\"column\"].astype(str).tolist()))\n","    exp += [\"label\", \"type\"]  # standardized targets if present\n","    return exp\n","\n","def _list_parquet_targets() -> Dict[str, str]:\n","    files = {}\n","    for dom in [\"Windows\", \"Linux\", \"Network\", \"IoT\"]:\n","        p = os.path.join(TRAIN_DIR, f\"train_ready_{dom}.parquet\")\n","        if os.path.isfile(p):\n","            files[dom] = p\n","    files[\"ALL\"] = os.path.join(TRAIN_DIR, \"train_ready_ALL.parquet\")\n","    return files\n","\n","def _pq_schema_cols(path: str) -> List[str]:\n","    pf = pq.ParquetFile(path)\n","    return [pf.schema_arrow.names[i] for i in range(len(pf.schema_arrow.names))]\n","\n","def _pq_rowcount(path: str) -> int:\n","    pf = pq.ParquetFile(path)\n","    return sum(pf.metadata.row_group(i).num_rows for i in range(pf.metadata.num_row_groups))\n","\n","def _value_counts_parquet(path: str, col: str, max_unique: int = 15) -> Tuple[int, Dict[str, int]]:\n","    pf = pq.ParquetFile(path)\n","    counts: Dict[str, int] = {}\n","    total = 0\n","    for i in range(pf.metadata.num_row_groups):\n","        tbl = pf.read_row_group(i, columns=[col]) if col in pf.schema_arrow.names else None\n","        if tbl is None:\n","            continue\n","        s = tbl.to_pandas()[col]\n","        total += s.notna().sum()\n","        vc = s.dropna().astype(str).value_counts()\n","        for k, v in vc.items():\n","            counts[k] = counts.get(k, 0) + int(v)\n","    # keep top max_unique\n","    counts = dict(sorted(counts.items(), key=lambda kv: kv[1], reverse=True)[:max_unique])\n","    return total, counts\n","\n","# ------------------------------ Main ----------------------------- #\n","def main() -> None:\n","    _ensure_inputs()\n","    expected_cols = _load_expected_columns()\n","    targets = {\"label\", \"type\"}\n","\n","    files = _list_parquet_targets()\n","    rows_summary = []\n","    rows_details = []\n","\n","    for dom, path in files.items():\n","        if not os.path.isfile(path):\n","            continue\n","\n","        cols = _pq_schema_cols(path)\n","        nrows = _pq_rowcount(path)\n","\n","        missing_expected = [c for c in expected_cols if c not in cols]\n","        unexpected = [c for c in cols if c not in expected_cols + [\"domain\"]]  # 'domain' may appear in ALL\n","\n","        has_label = \"label\" in cols\n","        has_type  = \"type\" in cols\n","\n","        label_nonnull = type_nonnull = 0\n","        label_vc = {}\n","        type_vc = {}\n","\n","        if has_label:\n","            label_nonnull, label_vc = _value_counts_parquet(path, \"label\")\n","        if has_type:\n","            type_nonnull, type_vc = _value_counts_parquet(path, \"type\")\n","\n","        rows_summary.append({\n","            \"dataset\": dom,\n","            \"path\": path,\n","            \"rows\": nrows,\n","            \"ncols\": len(cols),\n","            \"has_label\": has_label,\n","            \"has_type\": has_type,\n","            \"label_nonnull\": label_nonnull,\n","            \"type_nonnull\": type_nonnull,\n","            \"missing_expected_cols\": len(missing_expected),\n","            \"unexpected_cols\": len(unexpected),\n","        })\n","\n","        rows_details.append({\n","            \"dataset\": dom,\n","            \"path\": path,\n","            \"columns\": \",\".join(cols),\n","            \"missing_expected_cols\": \",\".join(missing_expected),\n","            \"unexpected_cols\": \",\".join(unexpected),\n","            \"label_top_values\": json.dumps(label_vc),\n","            \"type_top_values\": json.dumps(type_vc),\n","        })\n","\n","    pd.DataFrame(rows_summary).sort_values(\"dataset\").to_csv(SUMMARY_CSV, index=False)\n","    pd.DataFrame(rows_details).sort_values(\"dataset\").to_csv(DETAILS_CSV, index=False)\n","\n","    print(f\"validation_summary: {SUMMARY_CSV}\")\n","    print(f\"validation_details: {DETAILS_CSV}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZnl1wNQpqn4","executionInfo":{"status":"ok","timestamp":1762054950159,"user_tz":-300,"elapsed":116135,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"af5f592e-889d-4eb7-d353-eaf074a87b0a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["validation_summary: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/validation_summary.csv\n","validation_details: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/validation_details.csv\n","validation_summary: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/validation_summary.csv\n","validation_details: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/validation_details.csv\n","validation_summary: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/validation_summary.csv\n","validation_details: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/validation_details.csv\n","validation_summary: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/validation_summary.csv\n","validation_details: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_3/outputs/training_data/validation_details.csv\n"]}]}]}