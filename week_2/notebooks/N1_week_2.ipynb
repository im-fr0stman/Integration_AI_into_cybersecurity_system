{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPo82wFOJA6/AbkzVmJ+Jkh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gAoqU1lUY0Gv","executionInfo":{"status":"ok","timestamp":1762050832337,"user_tz":-300,"elapsed":25,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"6b52c5a7-e915-4b96-fdf0-7c51d079f986"},"outputs":[{"output_type":"stream","name":"stdout","text":["Week 2 initialized.\n","week_2 root:    /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_2\n","session meta:   /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_2/outputs/session_meta.json\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4049706581.py:83: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  created_at=datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n"]}],"source":["# /colab_notebooks/week2_step1_bootstrap_clean.ipynb\n","# Week 2 — Step 1: initialize week_2 structure and validate prerequisites (clean)\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass, asdict\n","from datetime import datetime\n","from typing import Dict\n","\n","# ----------------------------- Configuration ----------------------------- #\n","\n","DRIVE_MOUNT_PT: str = \"/content/drive\"\n","PROJECT_ROOT: str   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","DATASET_ROOT: str = os.path.join(PROJECT_ROOT, \"dataset\")  # shared across weeks\n","\n","W1_ROOT: str = os.path.join(PROJECT_ROOT, \"week_1\")\n","W1_OUT: str  = os.path.join(W1_ROOT, \"outputs\")\n","\n","W2_ROOT: str = os.path.join(PROJECT_ROOT, \"week_2\")\n","W2_NB: str   = os.path.join(W2_ROOT, \"notebooks\")\n","W2_OUT: str  = os.path.join(W2_ROOT, \"outputs\")\n","\n","REQUIRED_W1: Dict[str, str] = {\n","    \"inventory\": os.path.join(W1_OUT, \"data_inventory_paths.csv\"),\n","    \"schema\":    os.path.join(W1_OUT, \"schema_preview.json\"),\n","}\n","OPTIONAL_W1: Dict[str, str] = {\n","    \"summary\": os.path.join(W1_OUT, \"summary.csv\"),\n","    \"errors\":  os.path.join(W1_OUT, \"schema_errors.csv\"),\n","}\n","\n","\n","@dataclass(frozen=True)\n","class SessionMeta:\n","    project_root: str\n","    dataset_root: str\n","    week_1_outputs: Dict[str, str]\n","    week_2: Dict[str, str]\n","    created_at: str\n","    notes: str\n","\n","def _ensure_drive_mounted() -> None:\n","    \"\"\"Ensure Google Drive is mounted. Avoids interactive prompts if already mounted.\"\"\"\n","    import google.colab  # type: ignore\n","    from google.colab import drive  # type: ignore\n","    if not os.path.ismount(DRIVE_MOUNT_PT):\n","        drive.mount(DRIVE_MOUNT_PT)\n","\n","\n","def _ensure_structure() -> None:\n","    \"\"\"Create week_2 directories; avoid side effects elsewhere.\"\"\"\n","    os.makedirs(W2_NB, exist_ok=True)\n","    os.makedirs(W2_OUT, exist_ok=True)\n","\n","\n","def _validate_prereqs() -> Dict[str, str]:\n","    \"\"\"Validate Week 1 required artifacts and return resolved paths map.\"\"\"\n","    missing = [k for k, p in REQUIRED_W1.items() if not os.path.isfile(p)]\n","    if missing:\n","        raise FileNotFoundError(\n","            \"Week 2 prerequisites not found: \" + \", \".join(missing)\n","        )\n","    resolved = {k: v for k, v in REQUIRED_W1.items()}\n","    resolved.update({k: v for k, v in OPTIONAL_W1.items() if os.path.isfile(v)})\n","    return resolved\n","\n","\n","def _write_session_meta(resolved_w1: Dict[str, str]) -> str:\n","    \"\"\"Write session metadata JSON into week_2/outputs and return its path.\"\"\"\n","    meta = SessionMeta(\n","        project_root=PROJECT_ROOT,\n","        dataset_root=DATASET_ROOT,\n","        week_1_outputs=resolved_w1,\n","        week_2={\"root\": W2_ROOT, \"outputs\": W2_OUT, \"notebooks\": W2_NB},\n","        created_at=datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n","        notes=\"Week 2 initialized.\",\n","    )\n","    out_path = os.path.join(W2_OUT, \"session_meta.json\")\n","    with open(out_path, \"w\") as f:\n","        json.dump(asdict(meta), f, indent=2)\n","    return out_path\n","\n","\n","def main() -> None:\n","    \"\"\"Entry point: mount, create week_2 structure, validate prereqs, persist meta.\"\"\"\n","    _ensure_drive_mounted()\n","    _ensure_structure()\n","    resolved = _validate_prereqs()\n","    meta_path = _write_session_meta(resolved)\n","    print(f\"Week 2 initialized.\")\n","    print(f\"week_2 root:    {W2_ROOT}\")\n","    print(f\"session meta:   {meta_path}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","source":["# /colab_notebooks/week2_step2_quick_eda_clean.ipynb\n","# Week 2 — Step 2:modular quick EDA\n","from __future__ import annotations\n","\n","import os\n","from dataclasses import dataclass\n","from typing import List, Optional, Dict\n","\n","import pandas as pd\n","\n","\n","# ----------------------------- Configuration ----------------------------- #\n","\n","DRIVE_MOUNT_PT: str = \"/content/drive\"\n","PROJECT_ROOT: str   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","\n","W1_OUT: str = os.path.join(PROJECT_ROOT, \"week_1\", \"outputs\")\n","W2_OUT: str = os.path.join(PROJECT_ROOT, \"week_2\", \"outputs\")\n","\n","INVENTORY_CSV: str   = os.path.join(W1_OUT, \"data_inventory_paths.csv\")\n","EDA_FILES_CSV: str   = os.path.join(W2_OUT, \"eda_files.csv\")\n","EDA_OVERVIEW_CSV: str= os.path.join(W2_OUT, \"eda_overview.csv\")\n","\n","MAX_SAMPLE_ROWS: int = 10000\n","\n","\n","# ----------------------------- Data Models -------------------------------- #\n","\n","@dataclass(frozen=True)\n","class FileEntry:\n","    domain: str\n","    path: str\n","    extension: str\n","\n","\n","# ------------------------------ Utilities --------------------------------- #\n","\n","def ensure_inputs() -> None:\n","    \"\"\"Validate required inputs and ensure output directory exists.\"\"\"\n","    if not os.path.isfile(INVENTORY_CSV):\n","        raise FileNotFoundError(f\"Required file not found: {INVENTORY_CSV}\")\n","    os.makedirs(W2_OUT, exist_ok=True)\n","\n","\n","def read_inventory() -> List[FileEntry]:\n","    \"\"\"Load Week 1 inventory and return file entries.\"\"\"\n","    df = pd.read_csv(INVENTORY_CSV)\n","    if df.empty:\n","        raise ValueError(\"Inventory contains no entries.\")\n","    return [\n","        FileEntry(\n","            domain=str(r[\"domain\"]),\n","            path=str(r[\"path\"]),\n","            extension=str(r[\"extension\"]).lower(),\n","        )\n","        for _, r in df.iterrows()\n","    ]\n","\n","\n","def read_sample(path: str, extension: str, nrows: int) -> Optional[pd.DataFrame]:\n","    \"\"\"Return a sampled DataFrame or None on controlled failure.\"\"\"\n","    try:\n","        if extension == \".csv\":\n","            return pd.read_csv(path, nrows=nrows, low_memory=True)\n","        if extension == \".parquet\":\n","            return pd.read_parquet(path).head(nrows)\n","        return None\n","    except Exception:\n","        return None\n","\n","\n","def compute_file_metrics(df: Optional[pd.DataFrame]) -> Dict[str, Optional[float]]:\n","    \"\"\"Compute sample_rows, ncols, and overall null % from a DataFrame.\"\"\"\n","    if df is None or df.empty:\n","        return {\"sample_rows\": pd.NA, \"ncols\": pd.NA, \"null_pct_overall\": pd.NA}\n","    nrows = int(min(len(df), MAX_SAMPLE_ROWS))\n","    ncols = int(df.shape[1])\n","    if nrows == 0 or ncols == 0:\n","        return {\"sample_rows\": nrows, \"ncols\": ncols, \"null_pct_overall\": 100.0}\n","    null_pct = float((df.isna().sum().sum() / (nrows * ncols)) * 100.0)\n","    return {\n","        \"sample_rows\": nrows,\n","        \"ncols\": ncols,\n","        \"null_pct_overall\": round(null_pct, 4),\n","    }\n","\n","\n","def build_per_file(entries: List[FileEntry]) -> pd.DataFrame:\n","    \"\"\"Compute per-file EDA metrics for all entries.\"\"\"\n","    rows = []\n","    for e in entries:\n","        df = read_sample(e.path, e.extension, MAX_SAMPLE_ROWS)\n","        metrics = compute_file_metrics(df)\n","        rows.append(\n","            {\n","                \"domain\": e.domain,\n","                \"path\": e.path,\n","                \"extension\": e.extension,\n","                **metrics,\n","            }\n","        )\n","    return pd.DataFrame(rows)\n","\n","\n","def aggregate_overview(per_file: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Aggregate domain-level EDA metrics.\"\"\"\n","    overview = (\n","        per_file.groupby(\"domain\", dropna=False)\n","        .agg(\n","            files=(\"path\", \"count\"),\n","            median_sample_rows=(\"sample_rows\", \"median\"),\n","            avg_ncols=(\"ncols\", \"mean\"),\n","            mean_null_pct=(\"null_pct_overall\", \"mean\"),\n","        )\n","        .round({\"avg_ncols\": 2, \"mean_null_pct\": 4})\n","        .reset_index()\n","    )\n","    return overview\n","\n","\n","def main() -> None:\n","    \"\"\"Entry point: produce EDA CSVs.\"\"\"\n","    ensure_inputs()\n","    entries = read_inventory()\n","    per_file = build_per_file(entries)\n","    overview = aggregate_overview(per_file)\n","\n","    per_file.to_csv(EDA_FILES_CSV, index=False)\n","    overview.to_csv(EDA_OVERVIEW_CSV, index=False)\n","\n","    # Minimal confirmations\n","    print(f\"Per-file EDA written: {EDA_FILES_CSV}\")\n","    print(f\"Overview EDA written: {EDA_OVERVIEW_CSV}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s6mbbEFuXMf4","executionInfo":{"status":"ok","timestamp":1762052529851,"user_tz":-300,"elapsed":1628,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"13391e17-f06f-45d1-dcf3-99af8e30fd88"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2810720160.py:64: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,20,22,27,30,39,46,47,50,52,53,55,57,59,60,61,62,63,72,73,74,75,77,80,81,85,96,99,101,102,103,104,106,107,108,110,112,113,114,115,116,117,118,119,120,121,122,123,124) have mixed types. Specify dtype option on import or set low_memory=False.\n","  return pd.read_csv(path, nrows=nrows, low_memory=True)\n"]},{"output_type":"stream","name":"stdout","text":["Per-file EDA written: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_2/outputs/eda_files.csv\n","Overview EDA written: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_2/outputs/eda_overview.csv\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2810720160.py:64: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,47,48,49,50,51,52,59,60,61,65,66,67,71,72,73,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,104,105,106,107,109,112,113,116,127,130,132) have mixed types. Specify dtype option on import or set low_memory=False.\n","  return pd.read_csv(path, nrows=nrows, low_memory=True)\n"]}]},{"cell_type":"code","source":["# /colab_notebooks/week2_step3_column_profile.ipynb\n","\n","from __future__ import annotations\n","\n","import os\n","from collections import Counter, defaultdict\n","from dataclasses import dataclass\n","from typing import Dict, Iterable, List, Optional\n","\n","import pandas as pd\n","\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","W1_OUT         = os.path.join(PROJECT_ROOT, \"week_1\", \"outputs\")\n","W2_OUT         = os.path.join(PROJECT_ROOT, \"week_2\", \"outputs\")\n","\n","INVENTORY_CSV       = os.path.join(W1_OUT, \"data_inventory_paths.csv\")\n","COLUMN_PROFILE_CSV  = os.path.join(W2_OUT, \"column_profile.csv\")\n","DTYPE_SUMMARY_CSV   = os.path.join(W2_OUT, \"dtype_summary.csv\")\n","\n","MAX_SAMPLE_ROWS = 10_000  # as requested\n","\n","@dataclass(frozen=True)\n","class FileEntry:\n","    domain: str\n","    path: str\n","    extension: str\n","\n","@dataclass\n","class FileSchema:\n","    domain: str\n","    path: str\n","    columns: List[str]\n","    dtypes: Dict[str, str]\n","\n","def ensure_inputs() -> None:\n","    if not os.path.isfile(INVENTORY_CSV):\n","        raise FileNotFoundError(f\"Required file not found: {INVENTORY_CSV}\")\n","    os.makedirs(W2_OUT, exist_ok=True)\n","\n","def read_inventory() -> List[FileEntry]:\n","    df = pd.read_csv(INVENTORY_CSV)\n","    if df.empty:\n","        raise ValueError(\"Inventory contains no entries.\")\n","    return [\n","        FileEntry(str(r[\"domain\"]), str(r[\"path\"]), str(r[\"extension\"]).lower())\n","        for _, r in df.iterrows()\n","    ]\n","\n","def read_sample(path: str, extension: str, nrows: int) -> Optional[pd.DataFrame]:\n","    try:\n","        if extension == \".csv\":\n","            return pd.read_csv(path, nrows=nrows, low_memory=True)\n","        if extension == \".parquet\":\n","            return pd.read_parquet(path).head(nrows)\n","        return None\n","    except Exception:\n","        return None\n","\n","def infer_file_schema(e: FileEntry) -> Optional[FileSchema]:\n","    df = read_sample(e.path, e.extension, MAX_SAMPLE_ROWS)\n","    if df is None or df.empty:\n","        return None\n","    cols = list(df.columns)\n","    dtypes = {c: str(df[c].dtype) for c in cols}\n","    return FileSchema(e.domain, e.path, cols, dtypes)\n","\n","def collect_schemas(entries: Iterable[FileEntry]) -> List[FileSchema]:\n","    out: List[FileSchema] = []\n","    for e in entries:\n","        fs = infer_file_schema(e)\n","        if fs is not None:\n","            out.append(fs)\n","    if not out:\n","        raise ValueError(\"No readable files were found during sampling.\")\n","    return out\n","\n","def build_column_profile(schemas: List[FileSchema]) -> pd.DataFrame:\n","    by_domain_files = defaultdict(set)\n","    by_domain_col_files = defaultdict(lambda: defaultdict(set))\n","    by_domain_col_dtype = defaultdict(lambda: defaultdict(Counter))\n","\n","    for fs in schemas:\n","        by_domain_files[fs.domain].add(fs.path)\n","        for c in fs.columns:\n","            by_domain_col_files[fs.domain][c].add(fs.path)\n","            by_domain_col_dtype[fs.domain][c].update([fs.dtypes.get(c, \"unknown\")])\n","\n","    rows: List[Dict[str, object]] = []\n","    for dom, files in by_domain_files.items():\n","        total = len(files)\n","        for col, file_set in by_domain_col_files[dom].items():\n","            modal_dtype = by_domain_col_dtype[dom][col].most_common(1)[0][0]\n","            rows.append({\n","                \"domain\": dom,\n","                \"column\": col,\n","                \"files_with_column\": len(file_set),\n","                \"total_files_in_domain\": total,\n","                \"pct_coverage\": round(len(file_set) / total * 100.0, 2) if total else 0.0,\n","                \"modal_dtype\": modal_dtype,\n","            })\n","\n","    df = pd.DataFrame(rows)\n","    if df.empty:\n","        df = pd.DataFrame(columns=[\"domain\",\"column\",\"files_with_column\",\"total_files_in_domain\",\"pct_coverage\",\"modal_dtype\"])\n","    return df.sort_values([\"domain\",\"pct_coverage\",\"column\"], ascending=[True, False, True]).reset_index(drop=True)\n","\n","def build_dtype_summary(schemas: List[FileSchema]) -> pd.DataFrame:\n","    by_domain = defaultdict(Counter)\n","    for fs in schemas:\n","        by_domain[fs.domain].update(fs.dtypes.values())\n","    rows = [{\"domain\": d, \"dtype\": t, \"count\": int(c)} for d, ctr in by_domain.items() for t, c in ctr.most_common()]\n","    df = pd.DataFrame(rows)\n","    if df.empty:\n","        df = pd.DataFrame(columns=[\"domain\",\"dtype\",\"count\"])\n","    return df.sort_values([\"domain\",\"count\"], ascending=[True, False]).reset_index(drop=True)\n","\n","def main() -> None:\n","    ensure_inputs()\n","    entries = read_inventory()\n","    schemas = collect_schemas(entries)\n","    column_profile = build_column_profile(schemas)\n","    dtype_summary = build_dtype_summary(schemas)\n","    column_profile.to_csv(COLUMN_PROFILE_CSV, index=False)\n","    dtype_summary.to_csv(DTYPE_SUMMARY_CSV, index=False)\n","    print(f\"column_profile: {COLUMN_PROFILE_CSV}\")\n","    print(f\"dtype_summary:  {DTYPE_SUMMARY_CSV}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RMuWSWIpbNpf","executionInfo":{"status":"ok","timestamp":1762052541306,"user_tz":-300,"elapsed":1603,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"a8526e9c-a2b3-4f68-c1b9-6025d45d2d8d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-623071738.py:53: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,20,22,27,30,39,46,47,50,52,53,55,57,59,60,61,62,63,72,73,74,75,77,80,81,85,96,99,101,102,103,104,106,107,108,110,112,113,114,115,116,117,118,119,120,121,122,123,124) have mixed types. Specify dtype option on import or set low_memory=False.\n","  return pd.read_csv(path, nrows=nrows, low_memory=True)\n"]},{"output_type":"stream","name":"stdout","text":["column_profile: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_2/outputs/column_profile.csv\n","dtype_summary:  /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_2/outputs/dtype_summary.csv\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-623071738.py:53: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10,11,12,13,14,15,47,48,49,50,51,52,59,60,61,65,66,67,71,72,73,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,104,105,106,107,109,112,113,116,127,130,132) have mixed types. Specify dtype option on import or set low_memory=False.\n","  return pd.read_csv(path, nrows=nrows, low_memory=True)\n"]}]},{"cell_type":"code","source":["# /colab_notebooks/week2_step4_target_availability.ipynb\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional, Tuple\n","\n","import pandas as pd\n","\n","try:\n","    import pyarrow as pa\n","    import pyarrow.parquet as pq\n","except Exception:\n","    raise ImportError(\"pyarrow is required for Parquet counting. Install with: pip install pyarrow\")\n","\n","# --- Paths ---\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","W1_OUT         = os.path.join(PROJECT_ROOT, \"week_1\", \"outputs\")\n","W2_OUT         = os.path.join(PROJECT_ROOT, \"week_2\", \"outputs\")\n","\n","INVENTORY_CSV  = os.path.join(W1_OUT, \"data_inventory_paths.csv\")\n","SCHEMA_JSON    = os.path.join(W1_OUT, \"schema_preview.json\")\n","OUT_CSV        = os.path.join(W2_OUT, \"target_availability.csv\")\n","\n","os.makedirs(W2_OUT, exist_ok=True)\n","\n","# --- Models ---\n","@dataclass(frozen=True)\n","class FileEntry:\n","    domain: str\n","    path: str\n","    extension: str\n","\n","@dataclass\n","class TargetCols:\n","    label: Optional[str]\n","    type_: Optional[str]\n","\n","\n","# --- Helpers ---\n","def _ensure_inputs() -> None:\n","    if not os.path.isfile(INVENTORY_CSV):\n","        raise FileNotFoundError(f\"Required file not found: {INVENTORY_CSV}\")\n","    if not os.path.isfile(SCHEMA_JSON):\n","        raise FileNotFoundError(f\"Required file not found: {SCHEMA_JSON}\")\n","\n","def _read_inventory() -> List[FileEntry]:\n","    df = pd.read_csv(INVENTORY_CSV)\n","    if df.empty:\n","        raise ValueError(\"Inventory contains no entries.\")\n","    return [FileEntry(str(r.domain), str(r.path), str(r.extension).lower()) for _, r in df.iterrows()]\n","\n","def _read_schema_map() -> Dict[str, List[str]]:\n","    data = json.load(open(SCHEMA_JSON))\n","    # Map file path -> original column names list\n","    m: Dict[str, List[str]] = {}\n","    for rec in data:\n","        if \"path\" in rec and \"columns\" in rec:\n","            m[str(rec[\"path\"])] = [str(c) for c in rec[\"columns\"]]\n","    return m\n","\n","def _find_target_cols(cols: List[str]) -> TargetCols:\n","    low = {c.lower(): c for c in cols}\n","    label = low.get(\"label\") or low.get(\"labels\") or low.get(\"target\") or low.get(\"class\")\n","    type_ = low.get(\"type\") or low.get(\"attack_cat\")\n","    return TargetCols(label=label, type_=type_)\n","\n","def _count_csv(path: str, target: TargetCols, chunksize: int = 100_000) -> Tuple[int, int, int]:\n","    usecols = [c for c in [target.label, target.type_] if c]\n","    total = lbl = typ = 0\n","    if usecols:\n","        for chunk in pd.read_csv(path, usecols=usecols, chunksize=chunksize, low_memory=True):\n","            total += len(chunk)\n","            if target.label and target.label in chunk:\n","                lbl += chunk[target.label].notna().sum()\n","            if target.type_ and target.type_ in chunk:\n","                typ += chunk[target.type_].notna().sum()\n","    else:\n","        for chunk in pd.read_csv(path, usecols=None, chunksize=chunksize, low_memory=True):\n","            total += len(chunk)\n","    return total, lbl, typ\n","\n","def _count_parquet(path: str, target: TargetCols) -> Tuple[int, int, int]:\n","    pf = pq.ParquetFile(path)\n","    total = lbl = typ = 0\n","    want = [c for c in [target.label, target.type_] if c]\n","    # If neither target present, count total rows via metadata only.\n","    if not want:\n","        # Sum row counts from row groups without materializing\n","        return sum(pf.metadata.row_group(i).num_rows for i in range(pf.metadata.num_row_groups)), 0, 0\n","    for i in range(pf.metadata.num_row_groups):\n","        cols = want\n","        table = pf.read_row_group(i, columns=cols)\n","        total += table.num_rows\n","        for name in cols:\n","            arr = table.column(name)\n","            # Non-null = length - null_count\n","            non_null = arr.length() - arr.null_count\n","            if name == target.label:\n","                lbl += int(non_null)\n","            if name == target.type_:\n","                typ += int(non_null)\n","    return total, lbl, typ\n","\n","def _count_file(e: FileEntry, tcols: TargetCols) -> Tuple[int, int, int]:\n","    if e.extension == \".csv\":\n","        return _count_csv(e.path, tcols)\n","    if e.extension == \".parquet\":\n","        return _count_parquet(e.path, tcols)\n","    return (0, 0, 0)\n","\n","\n","# --- Main ---\n","def main() -> None:\n","    _ensure_inputs()\n","    entries = _read_inventory()\n","    schema_map = _read_schema_map()\n","\n","    rows = []\n","    for e in entries:\n","        cols = schema_map.get(e.path, [])\n","        tcols = _find_target_cols(cols)\n","        has_label = tcols.label is not None\n","        has_type  = tcols.type_ is not None\n","\n","        try:\n","            total, lbl, typ = _count_file(e, tcols)\n","        except Exception:\n","            total, lbl, typ = (0, 0, 0)\n","\n","        rows.append({\n","            \"domain\": e.domain,\n","            \"path\": e.path,\n","            \"extension\": e.extension,\n","            \"has_label_col\": has_label,\n","            \"has_type_col\": has_type,\n","            \"total_rows\": int(total),\n","            \"label_nonnull_rows\": int(lbl),\n","            \"type_nonnull_rows\": int(typ),\n","        })\n","\n","    per_file = pd.DataFrame(rows)\n","\n","    agg = (\n","        per_file.groupby(\"domain\", dropna=False)\n","        .agg(\n","            total_files=(\"path\", \"count\"),\n","            files_with_label_col=(\"has_label_col\", \"sum\"),\n","            files_with_type_col=(\"has_type_col\", \"sum\"),\n","            total_rows=(\"total_rows\", \"sum\"),\n","            label_nonnull_rows=(\"label_nonnull_rows\", \"sum\"),\n","            type_nonnull_rows=(\"type_nonnull_rows\", \"sum\"),\n","        )\n","        .reset_index()\n","    )\n","\n","    # Coverage percentages (row-level)\n","    agg[\"label_row_coverage_pct\"] = (\n","        (agg[\"label_nonnull_rows\"] / agg[\"total_rows\"]).where(agg[\"total_rows\"] > 0, 0.0) * 100\n","    ).round(2)\n","    agg[\"type_row_coverage_pct\"] = (\n","        (agg[\"type_nonnull_rows\"] / agg[\"total_rows\"]).where(agg[\"total_rows\"] > 0, 0.0) * 100\n","    ).round(2)\n","\n","    agg.to_csv(OUT_CSV, index=False)\n","    print(f\"target_availability: {OUT_CSV}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhlHAtWycZkT","executionInfo":{"status":"ok","timestamp":1762052575046,"user_tz":-300,"elapsed":26100,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"9bfcd51b-581c-4a3e-e05a-714badee928f"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["target_availability: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_2/outputs/target_availability.csv\n"]}]},{"cell_type":"code","source":["# /colab_notebooks/week2_step5_validation_and_report.ipynb\n","# Week 2 — Step 5: validations + concise Markdown report\n","\n","from __future__ import annotations\n","\n","import os\n","import json\n","from dataclasses import dataclass, asdict\n","from typing import Dict, List\n","\n","import pandas as pd\n","\n","# ----------------------------- Paths ----------------------------- #\n","DRIVE_MOUNT_PT = \"/content/drive\"\n","PROJECT_ROOT   = f\"{DRIVE_MOUNT_PT}/MyDrive/Colab Notebooks/New_cyber_project\"\n","W2_OUT         = os.path.join(PROJECT_ROOT, \"week_2\", \"outputs\")\n","\n","EDA_OVERVIEW_CSV = os.path.join(W2_OUT, \"eda_overview.csv\")\n","COLUMN_PROFILE_CSV = os.path.join(W2_OUT, \"column_profile.csv\")\n","DTYPE_SUMMARY_CSV = os.path.join(W2_OUT, \"dtype_summary.csv\")\n","TARGET_AVAIL_CSV = os.path.join(W2_OUT, \"target_availability.csv\")\n","\n","VALIDATION_JSON = os.path.join(W2_OUT, \"validation_report.json\")\n","REPORT_MD       = os.path.join(W2_OUT, \"week2_report.md\")\n","\n","os.makedirs(W2_OUT, exist_ok=True)\n","\n","# --------------------------- Parameters --------------------------- #\n","NULL_OK_THRESHOLD = 95.0         # mean null % must be <= this\n","COVERAGE_COL_PCT  = 50.0         # \"well-covered\" column threshold\n","MIN_WELL_COVERED  = 5            # require at least this many columns per domain\n","\n","# ----------------------------- Models ----------------------------- #\n","@dataclass\n","class DomainValidation:\n","    domain: str\n","    has_any_rows: bool\n","    has_any_targets: bool\n","    mean_null_pct_ok: bool\n","    well_covered_columns: int\n","    well_covered_ok: bool\n","    details: Dict[str, float | int]\n","\n","@dataclass\n","class ValidationResult:\n","    overall_pass: bool\n","    per_domain: List[DomainValidation]\n","\n","# --------------------------- Load Inputs -------------------------- #\n","for p in [EDA_OVERVIEW_CSV, COLUMN_PROFILE_CSV, DTYPE_SUMMARY_CSV, TARGET_AVAIL_CSV]:\n","    if not os.path.isfile(p):\n","        raise FileNotFoundError(f\"Required file not found: {p}\")\n","\n","eda = pd.read_csv(EDA_OVERVIEW_CSV)               # domain, files, median_sample_rows, avg_ncols, mean_null_pct\n","colprof = pd.read_csv(COLUMN_PROFILE_CSV)         # domain, column, files_with_column, total_files_in_domain, pct_coverage, modal_dtype\n","dtype_sum = pd.read_csv(DTYPE_SUMMARY_CSV)        # domain, dtype, count\n","tgt = pd.read_csv(TARGET_AVAIL_CSV)               # domain, totals + target availability\n","\n","# -------------------------- Derive Metrics ------------------------ #\n","# Well-covered columns per domain (>= COVERAGE_COL_PCT)\n","wc = (\n","    colprof[colprof[\"pct_coverage\"] >= COVERAGE_COL_PCT]\n","    .groupby(\"domain\")[\"column\"].nunique()\n","    .rename(\"well_covered_columns\")\n","    .to_frame()\n",")\n","\n","# Merge EDA + target availability + well-covered counts\n","summary = (\n","    eda.merge(\n","        tgt[\n","            [\n","                \"domain\",\n","                \"total_files\",\n","                \"files_with_label_col\",\n","                \"files_with_type_col\",\n","                \"total_rows\",\n","                \"label_nonnull_rows\",\n","                \"type_nonnull_rows\",\n","                \"label_row_coverage_pct\",\n","                \"type_row_coverage_pct\",\n","            ]\n","        ],\n","        on=\"domain\",\n","        how=\"left\",\n","    )\n","    .merge(wc, on=\"domain\", how=\"left\")\n","    .fillna({\"well_covered_columns\": 0})\n",")\n","\n","# --------------------------- Validation --------------------------- #\n","\n","per_domain_results: List[DomainValidation] = []\n","for _, r in summary.iterrows():\n","    has_rows = (r.get(\"total_rows\", 0) or 0) > 0\n","    has_targets = (r.get(\"files_with_label_col\", 0) or 0) > 0 or (r.get(\"files_with_type_col\", 0) or 0) > 0\n","    null_ok = float(r.get(\"mean_null_pct\", 100.0)) <= NULL_OK_THRESHOLD\n","    wc_count = int(r.get(\"well_covered_columns\", 0))\n","    wc_ok = wc_count >= MIN_WELL_COVERED\n","\n","    per_domain_results.append(\n","        DomainValidation(\n","            domain=str(r[\"domain\"]),\n","            has_any_rows=bool(has_rows),\n","            has_any_targets=bool(has_targets),\n","            mean_null_pct_ok=bool(null_ok),\n","            well_covered_columns=wc_count,\n","            well_covered_ok=bool(wc_ok),\n","            details={\n","                \"files\": int(r.get(\"files\", 0)),\n","                \"total_files\": int(r.get(\"total_files\", 0)),\n","                \"avg_ncols\": float(r.get(\"avg_ncols\", 0.0)),\n","                \"median_sample_rows\": float(r.get(\"median_sample_rows\", 0.0)),\n","                \"mean_null_pct\": float(r.get(\"mean_null_pct\", 0.0)),\n","                \"label_row_coverage_pct\": float(r.get(\"label_row_coverage_pct\", 0.0)),\n","                \"type_row_coverage_pct\": float(r.get(\"type_row_coverage_pct\", 0.0)),\n","            },\n","        )\n","    )\n","\n","overall_pass = all(\n","    d.has_any_rows and d.has_any_targets and d.mean_null_pct_ok and d.well_covered_ok\n","    for d in per_domain_results\n",")\n","\n","# --------------------------- Save JSON ---------------------------- #\n","val_result = ValidationResult(overall_pass=overall_pass, per_domain=per_domain_results)\n","with open(VALIDATION_JSON, \"w\") as f:\n","    json.dump(\n","        {\n","            \"overall_pass\": val_result.overall_pass,\n","            \"per_domain\": [\n","                {\n","                    \"domain\": d.domain,\n","                    \"has_any_rows\": d.has_any_rows,\n","                    \"has_any_targets\": d.has_any_targets,\n","                    \"mean_null_pct_ok\": d.mean_null_pct_ok,\n","                    \"well_covered_columns\": d.well_covered_columns,\n","                    \"well_covered_ok\": d.well_covered_ok,\n","                    \"details\": d.details,\n","                }\n","                for d in val_result.per_domain\n","            ],\n","            \"thresholds\": {\n","                \"NULL_OK_THRESHOLD\": NULL_OK_THRESHOLD,\n","                \"COVERAGE_COL_PCT\": COVERAGE_COL_PCT,\n","                \"MIN_WELL_COVERED\": MIN_WELL_COVERED,\n","            },\n","        },\n","        f,\n","        indent=2,\n","    )\n","\n","# --------------------------- Write Report ------------------------- #\n","lines = []\n","lines.append(f\"# Week 2 — Validation Report\")\n","lines.append(\"\")\n","lines.append(f\"**Overall status:** {'PASS' if overall_pass else 'FAIL'}\")\n","lines.append(\"\")\n","for d in per_domain_results:\n","    status = \"PASS\" if (d.has_any_rows and d.has_any_targets and d.mean_null_pct_ok and d.well_covered_ok) else \"FAIL\"\n","    lines.append(f\"## {d.domain} — {status}\")\n","    lines.append(f\"- files (sampled): {int(d.details['files'])}\")\n","    lines.append(f\"- total_files (domain): {int(d.details['total_files'])}\")\n","    lines.append(f\"- avg_ncols: {d.details['avg_ncols']:.2f}\")\n","    lines.append(f\"- median_sample_rows: {int(d.details['median_sample_rows']) if d.details['median_sample_rows']==int(d.details['median_sample_rows']) else d.details['median_sample_rows']}\")\n","    lines.append(f\"- mean_null_pct: {d.details['mean_null_pct']:.2f}%\")\n","    lines.append(f\"- label_row_coverage_pct: {d.details['label_row_coverage_pct']:.2f}%\")\n","    lines.append(f\"- type_row_coverage_pct: {d.details['type_row_coverage_pct']:.2f}%\")\n","    lines.append(f\"- well_covered_columns (≥{COVERAGE_COL_PCT:.0f}%): {d.well_covered_columns}\")\n","    lines.append(f\"- checks: rows={d.has_any_rows}, targets={d.has_any_targets}, nulls_ok={d.mean_null_pct_ok}, coverage_ok={d.well_covered_ok}\")\n","    lines.append(\"\")\n","\n","with open(REPORT_MD, \"w\") as f:\n","    f.write(\"\\n\".join(lines))\n","\n","print(f\"validation_report: {VALIDATION_JSON}\")\n","print(f\"markdown_report:  {REPORT_MD}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EclfCjLde58S","executionInfo":{"status":"ok","timestamp":1762052980640,"user_tz":-300,"elapsed":72,"user":{"displayName":"Ислом Махмудов","userId":"01836210873718254272"}},"outputId":"f6ecd7b3-060a-4cbd-e4b5-1080721cc3a2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["validation_report: /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_2/outputs/validation_report.json\n","markdown_report:  /content/drive/MyDrive/Colab Notebooks/New_cyber_project/week_2/outputs/week2_report.md\n"]}]}]}